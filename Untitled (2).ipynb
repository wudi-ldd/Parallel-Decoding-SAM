{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cdfcfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datapool/home/ph_teacher3/lwy/.lico_env/jupyter/env/lib/python3.9/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n",
      "Epoch 1/100 [Train]: 100%|██████████| 342/342 [04:34<00:00,  1.24it/s]\n",
      "Epoch 1/100 [Validation]: 100%|██████████| 95/95 [00:35<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.6515, Val Loss: 1.3061, (SegHead) IoU: 0.6490, F1: 0.7871, Precision: 0.8795, Recall: 0.7123, (Final) IoU: 0.6576, F1: 0.7935, Precision: 0.8529, Recall: 0.7418, Composite Score: 0.7614, LR: 0.000033\n",
      "Best model saved at epoch 1 with Composite Score 0.7614 using MLA strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 [Train]: 100%|██████████| 342/342 [04:18<00:00,  1.33it/s]\n",
      "Epoch 2/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 1.2577, Val Loss: 1.2697, (SegHead) IoU: 0.6491, F1: 0.7872, Precision: 0.8892, Recall: 0.7062, (Final) IoU: 0.6608, F1: 0.7958, Precision: 0.8525, Recall: 0.7461, Composite Score: 0.7638, LR: 0.000067\n",
      "Best model saved at epoch 2 with Composite Score 0.7638 using MLA strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 [Train]: 100%|██████████| 342/342 [04:16<00:00,  1.33it/s]\n",
      "Epoch 3/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 1.1467, Val Loss: 1.2557, (SegHead) IoU: 0.6644, F1: 0.7984, Precision: 0.8709, Recall: 0.7370, (Final) IoU: 0.6717, F1: 0.8036, Precision: 0.8558, Recall: 0.7575, Composite Score: 0.7722, LR: 0.000100\n",
      "Best model saved at epoch 3 with Composite Score 0.7722 using MLA strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 [Train]: 100%|██████████| 342/342 [04:17<00:00,  1.33it/s]\n",
      "Epoch 4/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Loss: 1.0001, Val Loss: 1.5306, (SegHead) IoU: 0.6172, F1: 0.7633, Precision: 0.9111, Recall: 0.6567, (Final) IoU: 0.6307, F1: 0.7736, Precision: 0.8831, Recall: 0.6882, Composite Score: 0.7439, LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 [Train]: 100%|██████████| 342/342 [04:16<00:00,  1.34it/s]\n",
      "Epoch 5/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 0.8858, Val Loss: 1.0226, (SegHead) IoU: 0.7085, F1: 0.8294, Precision: 0.7666, Recall: 0.9034, (Final) IoU: 0.6758, F1: 0.8065, Precision: 0.7342, Recall: 0.8947, Composite Score: 0.7778, LR: 0.000100\n",
      "Best model saved at epoch 5 with Composite Score 0.7778 using MLA strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 6/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Train Loss: 0.7924, Val Loss: 1.2922, (SegHead) IoU: 0.6640, F1: 0.7981, Precision: 0.8580, Recall: 0.7460, (Final) IoU: 0.6676, F1: 0.8007, Precision: 0.8183, Recall: 0.7838, Composite Score: 0.7676, LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 7/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Train Loss: 0.6864, Val Loss: 1.1906, (SegHead) IoU: 0.6868, F1: 0.8143, Precision: 0.8127, Recall: 0.8160, (Final) IoU: 0.6734, F1: 0.8048, Precision: 0.7891, Recall: 0.8212, Composite Score: 0.7722, LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 [Train]: 100%|██████████| 342/342 [04:45<00:00,  1.20it/s]\n",
      "Epoch 8/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Train Loss: 0.6141, Val Loss: 1.0749, (SegHead) IoU: 0.6922, F1: 0.8181, Precision: 0.8454, Recall: 0.7926, (Final) IoU: 0.6885, F1: 0.8155, Precision: 0.8257, Recall: 0.8055, Composite Score: 0.7838, LR: 0.000100\n",
      "Best model saved at epoch 8 with Composite Score 0.7838 using MLA strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 9/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Train Loss: 0.5980, Val Loss: 1.3361, (SegHead) IoU: 0.6600, F1: 0.7952, Precision: 0.8923, Recall: 0.7171, (Final) IoU: 0.6731, F1: 0.8046, Precision: 0.8635, Recall: 0.7532, Composite Score: 0.7736, LR: 0.000099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 10/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 0.5016, Val Loss: 1.4809, (SegHead) IoU: 0.6349, F1: 0.7767, Precision: 0.9026, Recall: 0.6816, (Final) IoU: 0.6445, F1: 0.7838, Precision: 0.8836, Recall: 0.7043, Composite Score: 0.7540, LR: 0.000099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 11/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Train Loss: 0.4655, Val Loss: 1.6167, (SegHead) IoU: 0.6334, F1: 0.7755, Precision: 0.9019, Recall: 0.6803, (Final) IoU: 0.6501, F1: 0.7879, Precision: 0.8751, Recall: 0.7165, Composite Score: 0.7574, LR: 0.000099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 12/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Train Loss: 0.4312, Val Loss: 1.5619, (SegHead) IoU: 0.6231, F1: 0.7678, Precision: 0.8986, Recall: 0.6702, (Final) IoU: 0.6473, F1: 0.7859, Precision: 0.8696, Recall: 0.7169, Composite Score: 0.7549, LR: 0.000098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 13/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Train Loss: 0.4753, Val Loss: 1.1984, (SegHead) IoU: 0.6794, F1: 0.8091, Precision: 0.8851, Recall: 0.7451, (Final) IoU: 0.6811, F1: 0.8103, Precision: 0.8575, Recall: 0.7680, Composite Score: 0.7792, LR: 0.000098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 14/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Train Loss: 0.4062, Val Loss: 1.3515, (SegHead) IoU: 0.6604, F1: 0.7955, Precision: 0.8950, Recall: 0.7158, (Final) IoU: 0.6719, F1: 0.8038, Precision: 0.8720, Recall: 0.7455, Composite Score: 0.7733, LR: 0.000097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.35it/s]\n",
      "Epoch 15/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Train Loss: 0.3573, Val Loss: 1.1234, (SegHead) IoU: 0.6912, F1: 0.8174, Precision: 0.8839, Recall: 0.7602, (Final) IoU: 0.6993, F1: 0.8231, Precision: 0.8593, Recall: 0.7897, Composite Score: 0.7929, LR: 0.000097\n",
      "Best model saved at epoch 15 with Composite Score 0.7929 using MLA strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 16/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Train Loss: 0.3373, Val Loss: 1.2265, (SegHead) IoU: 0.6770, F1: 0.8074, Precision: 0.9018, Recall: 0.7309, (Final) IoU: 0.6967, F1: 0.8213, Precision: 0.8707, Recall: 0.7772, Composite Score: 0.7915, LR: 0.000096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 17/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Train Loss: 0.3089, Val Loss: 1.1013, (SegHead) IoU: 0.6958, F1: 0.8206, Precision: 0.8920, Recall: 0.7599, (Final) IoU: 0.7128, F1: 0.8324, Precision: 0.8568, Recall: 0.8092, Composite Score: 0.8028, LR: 0.000096\n",
      "Best model saved at epoch 17 with Composite Score 0.8028 using MLA strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 [Train]: 100%|██████████| 342/342 [04:44<00:00,  1.20it/s]\n",
      "Epoch 18/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Train Loss: 0.3033, Val Loss: 1.1590, (SegHead) IoU: 0.6799, F1: 0.8095, Precision: 0.8843, Recall: 0.7463, (Final) IoU: 0.6949, F1: 0.8200, Precision: 0.8535, Recall: 0.7890, Composite Score: 0.7894, LR: 0.000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 [Train]: 100%|██████████| 342/342 [04:13<00:00,  1.35it/s]\n",
      "Epoch 19/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Train Loss: 0.2902, Val Loss: 1.2674, (SegHead) IoU: 0.6752, F1: 0.8061, Precision: 0.9001, Recall: 0.7299, (Final) IoU: 0.6951, F1: 0.8201, Precision: 0.8728, Recall: 0.7734, Composite Score: 0.7904, LR: 0.000094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 20/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Train Loss: 0.2945, Val Loss: 1.1623, (SegHead) IoU: 0.6725, F1: 0.8042, Precision: 0.8924, Recall: 0.7318, (Final) IoU: 0.6935, F1: 0.8190, Precision: 0.8548, Recall: 0.7861, Composite Score: 0.7884, LR: 0.000094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.35it/s]\n",
      "Epoch 21/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Train Loss: 0.3200, Val Loss: 1.1296, (SegHead) IoU: 0.7102, F1: 0.8305, Precision: 0.8788, Recall: 0.7873, (Final) IoU: 0.7135, F1: 0.8328, Precision: 0.8510, Recall: 0.8153, Composite Score: 0.8032, LR: 0.000093\n",
      "Best model saved at epoch 21 with Composite Score 0.8032 using MLA strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 22/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Train Loss: 0.4230, Val Loss: 1.7168, (SegHead) IoU: 0.6027, F1: 0.7521, Precision: 0.9043, Recall: 0.6437, (Final) IoU: 0.6356, F1: 0.7772, Precision: 0.8680, Recall: 0.7036, Composite Score: 0.7461, LR: 0.000092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 [Train]: 100%|██████████| 342/342 [04:48<00:00,  1.19it/s]\n",
      "Epoch 23/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Train Loss: 0.4593, Val Loss: 1.2835, (SegHead) IoU: 0.6576, F1: 0.7934, Precision: 0.8742, Recall: 0.7264, (Final) IoU: 0.6658, F1: 0.7994, Precision: 0.8258, Recall: 0.7746, Composite Score: 0.7664, LR: 0.000091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 24/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Train Loss: 0.3293, Val Loss: 1.3940, (SegHead) IoU: 0.6543, F1: 0.7910, Precision: 0.8928, Recall: 0.7101, (Final) IoU: 0.6703, F1: 0.8026, Precision: 0.8568, Recall: 0.7549, Composite Score: 0.7712, LR: 0.000090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 25/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Train Loss: 0.2669, Val Loss: 1.1154, (SegHead) IoU: 0.6990, F1: 0.8229, Precision: 0.8651, Recall: 0.7846, (Final) IoU: 0.7078, F1: 0.8289, Precision: 0.8336, Recall: 0.8244, Composite Score: 0.7987, LR: 0.000089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.35it/s]\n",
      "Epoch 26/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Train Loss: 0.2459, Val Loss: 1.2932, (SegHead) IoU: 0.6742, F1: 0.8054, Precision: 0.8860, Recall: 0.7383, (Final) IoU: 0.6872, F1: 0.8146, Precision: 0.8530, Recall: 0.7794, Composite Score: 0.7836, LR: 0.000088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 27/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Train Loss: 0.2333, Val Loss: 1.2508, (SegHead) IoU: 0.6831, F1: 0.8117, Precision: 0.8796, Recall: 0.7536, (Final) IoU: 0.6941, F1: 0.8195, Precision: 0.8494, Recall: 0.7916, Composite Score: 0.7886, LR: 0.000087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 [Train]: 100%|██████████| 342/342 [04:13<00:00,  1.35it/s]\n",
      "Epoch 28/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Train Loss: 0.2329, Val Loss: 1.3393, (SegHead) IoU: 0.6740, F1: 0.8052, Precision: 0.8807, Recall: 0.7417, (Final) IoU: 0.6815, F1: 0.8106, Precision: 0.8457, Recall: 0.7783, Composite Score: 0.7790, LR: 0.000086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.35it/s]\n",
      "Epoch 29/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Train Loss: 0.2263, Val Loss: 1.3972, (SegHead) IoU: 0.6614, F1: 0.7962, Precision: 0.8879, Recall: 0.7217, (Final) IoU: 0.6751, F1: 0.8060, Precision: 0.8519, Recall: 0.7648, Composite Score: 0.7745, LR: 0.000085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 [Train]: 100%|██████████| 342/342 [04:13<00:00,  1.35it/s]\n",
      "Epoch 30/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Train Loss: 0.2250, Val Loss: 1.3971, (SegHead) IoU: 0.6633, F1: 0.7976, Precision: 0.8866, Recall: 0.7248, (Final) IoU: 0.6820, F1: 0.8109, Precision: 0.8529, Recall: 0.7729, Composite Score: 0.7797, LR: 0.000083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 31/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Train Loss: 0.2254, Val Loss: 1.3000, (SegHead) IoU: 0.6721, F1: 0.8039, Precision: 0.8844, Recall: 0.7369, (Final) IoU: 0.6897, F1: 0.8164, Precision: 0.8520, Recall: 0.7836, Composite Score: 0.7854, LR: 0.000082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.35it/s]\n",
      "Epoch 32/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Train Loss: 0.2342, Val Loss: 1.5565, (SegHead) IoU: 0.6431, F1: 0.7828, Precision: 0.9036, Recall: 0.6905, (Final) IoU: 0.6691, F1: 0.8017, Precision: 0.8714, Recall: 0.7424, Composite Score: 0.7712, LR: 0.000081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.35it/s]\n",
      "Epoch 33/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Train Loss: 0.2275, Val Loss: 1.1889, (SegHead) IoU: 0.6977, F1: 0.8220, Precision: 0.8724, Recall: 0.7770, (Final) IoU: 0.7110, F1: 0.8311, Precision: 0.8360, Recall: 0.8262, Composite Score: 0.8011, LR: 0.000080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 34/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Train Loss: 0.2229, Val Loss: 1.2334, (SegHead) IoU: 0.6875, F1: 0.8148, Precision: 0.8831, Recall: 0.7563, (Final) IoU: 0.7049, F1: 0.8269, Precision: 0.8470, Recall: 0.8078, Composite Score: 0.7966, LR: 0.000078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 35/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Train Loss: 0.2203, Val Loss: 1.3696, (SegHead) IoU: 0.6671, F1: 0.8003, Precision: 0.8874, Recall: 0.7288, (Final) IoU: 0.6856, F1: 0.8135, Precision: 0.8517, Recall: 0.7786, Composite Score: 0.7824, LR: 0.000077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 36/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Train Loss: 0.2283, Val Loss: 1.7799, (SegHead) IoU: 0.6168, F1: 0.7630, Precision: 0.9181, Recall: 0.6527, (Final) IoU: 0.6543, F1: 0.7910, Precision: 0.8831, Recall: 0.7163, Composite Score: 0.7612, LR: 0.000076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.35it/s]\n",
      "Epoch 37/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Train Loss: 0.2247, Val Loss: 1.3468, (SegHead) IoU: 0.6727, F1: 0.8043, Precision: 0.8864, Recall: 0.7362, (Final) IoU: 0.6909, F1: 0.8172, Precision: 0.8463, Recall: 0.7901, Composite Score: 0.7861, LR: 0.000074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 38/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Train Loss: 0.2191, Val Loss: 1.3867, (SegHead) IoU: 0.6675, F1: 0.8006, Precision: 0.8922, Recall: 0.7261, (Final) IoU: 0.6944, F1: 0.8197, Precision: 0.8534, Recall: 0.7885, Composite Score: 0.7890, LR: 0.000073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 39/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Train Loss: 0.2177, Val Loss: 1.2339, (SegHead) IoU: 0.7012, F1: 0.8244, Precision: 0.8733, Recall: 0.7806, (Final) IoU: 0.7090, F1: 0.8297, Precision: 0.8361, Recall: 0.8234, Composite Score: 0.7995, LR: 0.000071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.35it/s]\n",
      "Epoch 40/100 [Validation]: 100%|██████████| 95/95 [00:33<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Train Loss: 0.2156, Val Loss: 1.5020, (SegHead) IoU: 0.6533, F1: 0.7903, Precision: 0.8998, Recall: 0.7046, (Final) IoU: 0.6804, F1: 0.8098, Precision: 0.8675, Recall: 0.7592, Composite Score: 0.7792, LR: 0.000070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 41/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Train Loss: 0.2253, Val Loss: 1.2638, (SegHead) IoU: 0.6930, F1: 0.8187, Precision: 0.8766, Recall: 0.7680, (Final) IoU: 0.7050, F1: 0.8270, Precision: 0.8338, Recall: 0.8203, Composite Score: 0.7965, LR: 0.000069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 [Train]: 100%|██████████| 342/342 [04:19<00:00,  1.32it/s]\n",
      "Epoch 42/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Train Loss: 0.2249, Val Loss: 1.3525, (SegHead) IoU: 0.6778, F1: 0.8080, Precision: 0.8832, Recall: 0.7446, (Final) IoU: 0.6967, F1: 0.8212, Precision: 0.8495, Recall: 0.7948, Composite Score: 0.7906, LR: 0.000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 [Train]: 100%|██████████| 342/342 [04:18<00:00,  1.32it/s]\n",
      "Epoch 43/100 [Validation]: 100%|██████████| 95/95 [00:38<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Train Loss: 0.2084, Val Loss: 1.2491, (SegHead) IoU: 0.6925, F1: 0.8183, Precision: 0.8815, Recall: 0.7635, (Final) IoU: 0.7035, F1: 0.8259, Precision: 0.8461, Recall: 0.8067, Composite Score: 0.7956, LR: 0.000065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 44/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Train Loss: 0.2775, Val Loss: 1.2006, (SegHead) IoU: 0.6836, F1: 0.8120, Precision: 0.8017, Recall: 0.8227, (Final) IoU: 0.6813, F1: 0.8104, Precision: 0.7590, Recall: 0.8694, Composite Score: 0.7800, LR: 0.000064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 [Train]: 100%|██████████| 342/342 [04:15<00:00,  1.34it/s]\n",
      "Epoch 45/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100], Train Loss: 0.2945, Val Loss: 1.9866, (SegHead) IoU: 0.6170, F1: 0.7631, Precision: 0.9090, Recall: 0.6576, (Final) IoU: 0.6457, F1: 0.7847, Precision: 0.8713, Recall: 0.7138, Composite Score: 0.7539, LR: 0.000062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 [Train]: 100%|██████████| 342/342 [04:13<00:00,  1.35it/s]\n",
      "Epoch 46/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Train Loss: 0.2354, Val Loss: 1.3302, (SegHead) IoU: 0.6767, F1: 0.8072, Precision: 0.8823, Recall: 0.7438, (Final) IoU: 0.6919, F1: 0.8179, Precision: 0.8502, Recall: 0.7881, Composite Score: 0.7870, LR: 0.000061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 [Train]: 100%|██████████| 342/342 [04:14<00:00,  1.34it/s]\n",
      "Epoch 47/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100], Train Loss: 0.2152, Val Loss: 1.4437, (SegHead) IoU: 0.6695, F1: 0.8020, Precision: 0.8891, Recall: 0.7304, (Final) IoU: 0.6908, F1: 0.8171, Precision: 0.8558, Recall: 0.7818, Composite Score: 0.7864, LR: 0.000059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 [Train]: 100%|██████████| 342/342 [06:13<00:00,  1.09s/it]\n",
      "Epoch 48/100 [Validation]: 100%|██████████| 95/95 [01:02<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Train Loss: 0.2088, Val Loss: 1.3992, (SegHead) IoU: 0.6776, F1: 0.8078, Precision: 0.8777, Recall: 0.7482, (Final) IoU: 0.6906, F1: 0.8170, Precision: 0.8453, Recall: 0.7905, Composite Score: 0.7858, LR: 0.000058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 [Train]: 100%|██████████| 342/342 [07:54<00:00,  1.39s/it]\n",
      "Epoch 49/100 [Validation]: 100%|██████████| 95/95 [01:03<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Train Loss: 0.2066, Val Loss: 1.3746, (SegHead) IoU: 0.6812, F1: 0.8104, Precision: 0.8756, Recall: 0.7542, (Final) IoU: 0.6937, F1: 0.8192, Precision: 0.8419, Recall: 0.7977, Composite Score: 0.7881, LR: 0.000056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 [Train]: 100%|██████████| 342/342 [07:54<00:00,  1.39s/it]\n",
      "Epoch 50/100 [Validation]: 100%|██████████| 95/95 [01:02<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Train Loss: 0.2040, Val Loss: 1.4519, (SegHead) IoU: 0.6747, F1: 0.8058, Precision: 0.8816, Recall: 0.7420, (Final) IoU: 0.6901, F1: 0.8167, Precision: 0.8486, Recall: 0.7870, Composite Score: 0.7856, LR: 0.000055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 [Train]: 100%|██████████| 342/342 [07:54<00:00,  1.39s/it]\n",
      "Epoch 51/100 [Validation]: 100%|██████████| 95/95 [01:02<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Train Loss: 0.2004, Val Loss: 1.4664, (SegHead) IoU: 0.6678, F1: 0.8008, Precision: 0.8853, Recall: 0.7311, (Final) IoU: 0.6864, F1: 0.8141, Precision: 0.8547, Recall: 0.7771, Composite Score: 0.7831, LR: 0.000053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 [Train]: 100%|██████████| 342/342 [07:55<00:00,  1.39s/it]\n",
      "Epoch 52/100 [Validation]: 100%|██████████| 95/95 [00:37<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Train Loss: 0.2056, Val Loss: 1.4843, (SegHead) IoU: 0.6672, F1: 0.8004, Precision: 0.8873, Recall: 0.7290, (Final) IoU: 0.6864, F1: 0.8140, Precision: 0.8528, Recall: 0.7787, Composite Score: 0.7830, LR: 0.000051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 [Train]: 100%|██████████| 342/342 [05:06<00:00,  1.12it/s]\n",
      "Epoch 53/100 [Validation]: 100%|██████████| 95/95 [01:02<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Train Loss: 0.2032, Val Loss: 1.5340, (SegHead) IoU: 0.6669, F1: 0.8002, Precision: 0.8938, Recall: 0.7243, (Final) IoU: 0.6871, F1: 0.8146, Precision: 0.8597, Recall: 0.7739, Composite Score: 0.7838, LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 [Train]: 100%|██████████| 342/342 [07:54<00:00,  1.39s/it]\n",
      "Epoch 54/100 [Validation]: 100%|██████████| 95/95 [01:02<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Train Loss: 0.2022, Val Loss: 1.6811, (SegHead) IoU: 0.6430, F1: 0.7827, Precision: 0.9064, Recall: 0.6888, (Final) IoU: 0.6753, F1: 0.8062, Precision: 0.8667, Recall: 0.7535, Composite Score: 0.7754, LR: 0.000048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 [Train]: 100%|██████████| 342/342 [07:54<00:00,  1.39s/it]\n",
      "Epoch 55/100 [Validation]: 100%|██████████| 95/95 [01:02<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100], Train Loss: 0.2023, Val Loss: 1.4971, (SegHead) IoU: 0.6703, F1: 0.8026, Precision: 0.8843, Recall: 0.7348, (Final) IoU: 0.6858, F1: 0.8136, Precision: 0.8485, Recall: 0.7815, Composite Score: 0.7824, LR: 0.000046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 [Train]: 100%|██████████| 342/342 [07:56<00:00,  1.39s/it]\n",
      "Epoch 56/100 [Validation]: 100%|██████████| 95/95 [01:03<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Train Loss: 0.2060, Val Loss: 1.4469, (SegHead) IoU: 0.6766, F1: 0.8071, Precision: 0.8809, Recall: 0.7447, (Final) IoU: 0.6906, F1: 0.8170, Precision: 0.8470, Recall: 0.7890, Composite Score: 0.7859, LR: 0.000045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 [Train]: 100%|██████████| 342/342 [07:57<00:00,  1.40s/it]\n",
      "Epoch 57/100 [Validation]: 100%|██████████| 95/95 [01:22<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Train Loss: 0.2014, Val Loss: 1.5490, (SegHead) IoU: 0.6584, F1: 0.7940, Precision: 0.8955, Recall: 0.7132, (Final) IoU: 0.6829, F1: 0.8116, Precision: 0.8596, Recall: 0.7687, Composite Score: 0.7807, LR: 0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 [Train]: 100%|██████████| 342/342 [05:07<00:00,  1.11it/s]\n",
      "Epoch 58/100 [Validation]: 100%|██████████| 95/95 [00:36<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Train Loss: 0.2046, Val Loss: 1.5196, (SegHead) IoU: 0.6673, F1: 0.8005, Precision: 0.8882, Recall: 0.7285, (Final) IoU: 0.6885, F1: 0.8155, Precision: 0.8560, Recall: 0.7787, Composite Score: 0.7847, LR: 0.000042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 [Train]: 100%|██████████| 342/342 [04:28<00:00,  1.28it/s]\n",
      "Epoch 59/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100], Train Loss: 0.1966, Val Loss: 1.4458, (SegHead) IoU: 0.6787, F1: 0.8086, Precision: 0.8803, Recall: 0.7477, (Final) IoU: 0.6940, F1: 0.8193, Precision: 0.8461, Recall: 0.7942, Composite Score: 0.7884, LR: 0.000040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 [Train]: 100%|██████████| 342/342 [04:13<00:00,  1.35it/s]\n",
      "Epoch 60/100 [Validation]: 100%|██████████| 95/95 [00:44<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Train Loss: 0.2033, Val Loss: 1.4215, (SegHead) IoU: 0.6809, F1: 0.8101, Precision: 0.8786, Recall: 0.7515, (Final) IoU: 0.6947, F1: 0.8198, Precision: 0.8424, Recall: 0.7985, Composite Score: 0.7888, LR: 0.000039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 [Train]: 100%|██████████| 342/342 [07:54<00:00,  1.39s/it]\n",
      "Epoch 61/100 [Validation]: 100%|██████████| 95/95 [01:03<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Train Loss: 0.1973, Val Loss: 1.4224, (SegHead) IoU: 0.6813, F1: 0.8104, Precision: 0.8780, Recall: 0.7526, (Final) IoU: 0.6978, F1: 0.8220, Precision: 0.8447, Recall: 0.8005, Composite Score: 0.7912, LR: 0.000037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 [Train]: 100%|██████████| 342/342 [07:55<00:00,  1.39s/it]\n",
      "Epoch 62/100 [Validation]: 100%|██████████| 95/95 [01:03<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Train Loss: 0.1992, Val Loss: 1.4009, (SegHead) IoU: 0.6856, F1: 0.8135, Precision: 0.8758, Recall: 0.7595, (Final) IoU: 0.7024, F1: 0.8252, Precision: 0.8359, Recall: 0.8147, Composite Score: 0.7945, LR: 0.000036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 [Train]: 100%|██████████| 342/342 [07:55<00:00,  1.39s/it]\n",
      "Epoch 63/100 [Validation]: 100%|██████████| 95/95 [01:03<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Train Loss: 0.2000, Val Loss: 1.4045, (SegHead) IoU: 0.6810, F1: 0.8102, Precision: 0.8757, Recall: 0.7538, (Final) IoU: 0.6985, F1: 0.8225, Precision: 0.8412, Recall: 0.8046, Composite Score: 0.7917, LR: 0.000034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 [Train]: 100%|██████████| 342/342 [07:55<00:00,  1.39s/it]\n",
      "Epoch 64/100 [Validation]: 100%|██████████| 95/95 [01:03<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Train Loss: 0.1968, Val Loss: 1.4885, (SegHead) IoU: 0.6746, F1: 0.8057, Precision: 0.8812, Recall: 0.7421, (Final) IoU: 0.6925, F1: 0.8183, Precision: 0.8453, Recall: 0.7930, Composite Score: 0.7873, LR: 0.000032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 [Train]: 100%|██████████| 342/342 [06:01<00:00,  1.06s/it]\n",
      "Epoch 65/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100], Train Loss: 0.2025, Val Loss: 1.4306, (SegHead) IoU: 0.6870, F1: 0.8145, Precision: 0.8729, Recall: 0.7634, (Final) IoU: 0.7019, F1: 0.8248, Precision: 0.8373, Recall: 0.8127, Composite Score: 0.7942, LR: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 [Train]: 100%|██████████| 342/342 [04:18<00:00,  1.32it/s]\n",
      "Epoch 66/100 [Validation]: 100%|██████████| 95/95 [00:36<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Train Loss: 0.1986, Val Loss: 1.5184, (SegHead) IoU: 0.6735, F1: 0.8049, Precision: 0.8834, Recall: 0.7392, (Final) IoU: 0.6946, F1: 0.8198, Precision: 0.8486, Recall: 0.7928, Composite Score: 0.7889, LR: 0.000030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 [Train]: 100%|██████████| 342/342 [07:37<00:00,  1.34s/it]\n",
      "Epoch 67/100 [Validation]: 100%|██████████| 95/95 [01:05<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100], Train Loss: 0.2002, Val Loss: 1.4512, (SegHead) IoU: 0.6824, F1: 0.8112, Precision: 0.8763, Recall: 0.7551, (Final) IoU: 0.6989, F1: 0.8228, Precision: 0.8435, Recall: 0.8030, Composite Score: 0.7920, LR: 0.000028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 [Train]: 100%|██████████| 342/342 [08:16<00:00,  1.45s/it]\n",
      "Epoch 68/100 [Validation]: 100%|██████████| 95/95 [01:03<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Train Loss: 0.1967, Val Loss: 1.5983, (SegHead) IoU: 0.6687, F1: 0.8015, Precision: 0.8887, Recall: 0.7299, (Final) IoU: 0.6908, F1: 0.8172, Precision: 0.8563, Recall: 0.7814, Composite Score: 0.7864, LR: 0.000027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 [Train]:   5%|▌         | 18/342 [00:25<07:35,  1.41s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 72/100 [Train]: 100%|██████████| 342/342 [08:00<00:00,  1.41s/it]\n",
      "Epoch 72/100 [Validation]: 100%|██████████| 95/95 [01:03<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Train Loss: 0.1998, Val Loss: 1.9140, (SegHead) IoU: 0.6422, F1: 0.7821, Precision: 0.9060, Recall: 0.6880, (Final) IoU: 0.6712, F1: 0.8033, Precision: 0.8676, Recall: 0.7478, Composite Score: 0.7725, LR: 0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 [Train]: 100%|██████████| 342/342 [08:01<00:00,  1.41s/it]\n",
      "Epoch 73/100 [Validation]: 100%|██████████| 95/95 [01:04<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100], Train Loss: 0.2009, Val Loss: 1.5208, (SegHead) IoU: 0.6783, F1: 0.8083, Precision: 0.8813, Recall: 0.7466, (Final) IoU: 0.6956, F1: 0.8205, Precision: 0.8456, Recall: 0.7969, Composite Score: 0.7896, LR: 0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 [Train]: 100%|██████████| 342/342 [08:03<00:00,  1.41s/it]\n",
      "Epoch 74/100 [Validation]: 100%|██████████| 95/95 [01:05<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Train Loss: 0.1967, Val Loss: 1.4408, (SegHead) IoU: 0.6933, F1: 0.8189, Precision: 0.8627, Recall: 0.7793, (Final) IoU: 0.7027, F1: 0.8254, Precision: 0.8281, Recall: 0.8227, Composite Score: 0.7948, LR: 0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 [Train]: 100%|██████████| 342/342 [07:37<00:00,  1.34s/it]\n",
      "Epoch 75/100 [Validation]: 100%|██████████| 95/95 [00:34<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100], Train Loss: 0.1955, Val Loss: 1.6509, (SegHead) IoU: 0.6708, F1: 0.8030, Precision: 0.8874, Recall: 0.7332, (Final) IoU: 0.6881, F1: 0.8152, Precision: 0.8555, Recall: 0.7786, Composite Score: 0.7843, LR: 0.000018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 [Train]: 100%|██████████| 342/342 [04:18<00:00,  1.32it/s]\n",
      "Epoch 76/100 [Validation]: 100%|██████████| 95/95 [00:35<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Train Loss: 0.1994, Val Loss: 1.6146, (SegHead) IoU: 0.6715, F1: 0.8034, Precision: 0.8832, Recall: 0.7369, (Final) IoU: 0.6907, F1: 0.8171, Precision: 0.8492, Recall: 0.7873, Composite Score: 0.7861, LR: 0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 [Train]: 100%|██████████| 342/342 [04:17<00:00,  1.33it/s]\n",
      "Epoch 77/100 [Validation]: 100%|██████████| 95/95 [00:36<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Train Loss: 0.1977, Val Loss: 1.5428, (SegHead) IoU: 0.6825, F1: 0.8113, Precision: 0.8774, Recall: 0.7545, (Final) IoU: 0.6972, F1: 0.8216, Precision: 0.8426, Recall: 0.8016, Composite Score: 0.7908, LR: 0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 [Train]: 100%|██████████| 342/342 [04:18<00:00,  1.32it/s]\n",
      "Epoch 78/100 [Validation]: 100%|██████████| 95/95 [00:35<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100], Train Loss: 0.1956, Val Loss: 1.5073, (SegHead) IoU: 0.6889, F1: 0.8158, Precision: 0.8670, Recall: 0.7703, (Final) IoU: 0.6997, F1: 0.8233, Precision: 0.8328, Recall: 0.8141, Composite Score: 0.7925, LR: 0.000014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 [Train]: 100%|██████████| 342/342 [04:18<00:00,  1.32it/s]\n",
      "Epoch 79/100 [Validation]: 100%|██████████| 95/95 [00:35<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100], Train Loss: 0.1963, Val Loss: 1.6585, (SegHead) IoU: 0.6706, F1: 0.8028, Precision: 0.8857, Recall: 0.7341, (Final) IoU: 0.6926, F1: 0.8184, Precision: 0.8527, Recall: 0.7867, Composite Score: 0.7876, LR: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 [Train]: 100%|██████████| 342/342 [04:16<00:00,  1.33it/s]\n",
      "Epoch 80/100 [Validation]: 100%|██████████| 95/95 [00:35<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Train Loss: 0.1974, Val Loss: 1.5981, (SegHead) IoU: 0.6769, F1: 0.8073, Precision: 0.8808, Recall: 0.7451, (Final) IoU: 0.6940, F1: 0.8193, Precision: 0.8474, Recall: 0.7931, Composite Score: 0.7885, LR: 0.000012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 [Train]:  33%|███▎      | 114/342 [01:24<02:50,  1.34it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import logging\n",
    "from segment_anything import sam_model_registry\n",
    "from tqdm import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from lora import LoRA_sam\n",
    "from types import MethodType\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 配置字典\n",
    "CONFIG = {\n",
    "    'dataset_name': 'beijing',  # 仅需设置数据集名称\n",
    "    'data_base_dir': 'datasets',  # 数据集的基目录\n",
    "    'log_dir_base': 'logs',        # 日志的基目录\n",
    "    'log_file': 'best_model_metrics.log',\n",
    "    'save_dir_base': 'logs',       # 模型保存的基目录\n",
    "    'save_prefix': 'best_model',\n",
    "    'model_type': 'vit_l',\n",
    "    'checkpoint': 'weights/sam_vit_l_0b3195.pth',\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'betas': (0.9, 0.999),\n",
    "    'weight_decay': 1e-4,\n",
    "    'metric_weights': {\n",
    "        'iou': 0.25,\n",
    "        'f1': 0.25,\n",
    "        'precision': 0.25,\n",
    "        'recall': 0.25\n",
    "    },\n",
    "    'aux_weight': 0.4,\n",
    "    'num_classes': 1,\n",
    "    'rank': 512,\n",
    "    'batch_size': 1\n",
    "}\n",
    "\n",
    "# # 更新随机种子\n",
    "set_seed(42)  # 42 是任意选择的种子值\n",
    "\n",
    "# 基于 dataset_name 构建路径\n",
    "image_dir = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'images')\n",
    "mask_dir = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'masks')\n",
    "train_txt = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'train.txt')\n",
    "val_txt = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'val.txt')\n",
    "\n",
    "# 定义能够返回中间层的 forward 方法\n",
    "def forward_inter(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = self.patch_embed(x)\n",
    "    if self.pos_embed is not None:\n",
    "        x = x + self.pos_embed\n",
    "    inter_features = []\n",
    "    for blk in self.blocks:\n",
    "        x = blk(x)\n",
    "        inter_features.append(x)\n",
    "\n",
    "    x = self.neck(x.permute(0, 3, 1, 2))\n",
    "    return x, inter_features\n",
    "\n",
    "# 更新日志和保存目录\n",
    "log_dir = os.path.join(CONFIG['log_dir_base'], CONFIG['dataset_name'])\n",
    "save_dir = os.path.join(CONFIG['save_dir_base'], CONFIG['dataset_name'])\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(log_dir, CONFIG['log_file']),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# 加载 SAM 模型并初始化 LoRA\n",
    "sam_model = sam_model_registry[CONFIG['model_type']](checkpoint=CONFIG['checkpoint'])\n",
    "sam_model.image_encoder.forward_inter = MethodType(forward_inter, sam_model.image_encoder)\n",
    "sam_model.to(CONFIG['device'])\n",
    "\n",
    "lora_sam_model = LoRA_sam(sam_model, rank=CONFIG['rank'])\n",
    "lora_sam_model.to(CONFIG['device'])\n",
    "\n",
    "# 读取训练集和验证集列表\n",
    "def read_split_files(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_names = f.read().strip().split('\\n')\n",
    "    return file_names\n",
    "\n",
    "# 数据集加载\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, sam_model, file_list, mask_size=(1024, 1024), device='cpu'):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.sam_model = sam_model\n",
    "        self.mask_size = mask_size\n",
    "        self.device = device\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png') and f.replace('.png', '') in file_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        mask_file = image_file\n",
    "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "        mask = cv2.resize(mask, self.mask_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        input_image_torch = torch.as_tensor(image, dtype=torch.float32).to(self.device)\n",
    "        input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()  # [C, H, W]\n",
    "\n",
    "        input_image = self.sam_model.preprocess(input_image_torch.to(self.device))\n",
    "\n",
    "        mask = torch.as_tensor(mask, dtype=torch.float32).to(self.device)  # 单通道浮点数\n",
    "\n",
    "        return input_image, mask\n",
    "\n",
    "# 读取文件列表\n",
    "train_files = read_split_files(train_txt)\n",
    "val_files = read_split_files(val_txt)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = SegmentationDataset(\n",
    "    image_dir=image_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    sam_model=sam_model,\n",
    "    file_list=train_files,\n",
    "    device=CONFIG['device']\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    image_dir=image_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    sam_model=sam_model,\n",
    "    file_list=val_files,\n",
    "    device=CONFIG['device']\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# def process_class_logits(class_logits):\n",
    "#     probs = torch.sigmoid(class_logits)\n",
    "#     binary_masks = (probs > 0.5).cpu().numpy().astype(np.uint8)  # [B,1,1024,1024]\n",
    "#     batch_results = []\n",
    "#     for batch_idx in range(binary_masks.shape[0]):\n",
    "#         current_mask = binary_masks[batch_idx, 0, :, :]\n",
    "#         num_labels, labels = cv2.connectedComponents(current_mask)\n",
    "#         sample_results = []\n",
    "#         for label in range(1, num_labels):\n",
    "#             current_component = (labels == label).astype(np.uint8)\n",
    "#             y_coords, x_coords = np.nonzero(current_component)\n",
    "\n",
    "#             min_x, max_x = np.min(x_coords), np.max(x_coords)\n",
    "#             min_y, max_y = np.min(y_coords), np.max(y_coords)\n",
    "\n",
    "#             mask = np.zeros_like(current_component, dtype=np.uint8)\n",
    "#             mask[min_y:max_y+1, min_x:max_x+1] = current_component[min_y:max_y+1, min_x:max_x+1]\n",
    "\n",
    "#             mask_resized = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#             sample_results.append({\n",
    "#                 'bbox': [min_x, min_y, max_x, max_y],\n",
    "#                 'mask': mask_resized\n",
    "#             })\n",
    "#         batch_results.append(sample_results)\n",
    "#     return batch_results\n",
    "\n",
    "def process_class_logits(class_logits):\n",
    "    probs = torch.sigmoid(class_logits)\n",
    "    binary_masks = (probs > 0.5).cpu().numpy().astype(np.uint8)  # [B,1,1024,1024]\n",
    "    image_size = 1024  # 图像的宽度和高度\n",
    "    batch_results = []\n",
    "\n",
    "    for batch_idx in range(binary_masks.shape[0]):\n",
    "        current_mask = binary_masks[batch_idx, 0, :, :]\n",
    "        num_labels, labels = cv2.connectedComponents(current_mask)\n",
    "        sample_results = []\n",
    "\n",
    "        for label in range(1, num_labels):\n",
    "            current_component = (labels == label).astype(np.uint8)\n",
    "            y_coords, x_coords = np.nonzero(current_component)\n",
    "\n",
    "            min_x, max_x = np.min(x_coords), np.max(x_coords)\n",
    "            min_y, max_y = np.min(y_coords), np.max(y_coords)\n",
    "\n",
    "            # 扩大边界框\n",
    "            bbox_width = max_x - min_x\n",
    "            bbox_height = max_y - min_y\n",
    "\n",
    "            expanded_min_x = max(0, int(min_x - 0.05 * bbox_width))\n",
    "            expanded_max_x = min(image_size - 1, int(max_x + 0.05 * bbox_width))\n",
    "            expanded_min_y = max(0, int(min_y - 0.05 * bbox_height))\n",
    "            expanded_max_y = min(image_size - 1, int(max_y + 0.05 * bbox_height))\n",
    "\n",
    "            mask = np.zeros_like(current_component, dtype=np.uint8)\n",
    "            mask[min_y:max_y+1, min_x:max_x+1] = current_component[min_y:max_y+1, min_x:max_x+1]\n",
    "\n",
    "            mask_resized = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            sample_results.append({\n",
    "                'bbox': [expanded_min_x, expanded_min_y, expanded_max_x, expanded_max_y],\n",
    "                'mask': mask_resized\n",
    "            })\n",
    "\n",
    "        batch_results.append(sample_results)\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "def predict_masks_batch(sam_model, image_embeddings, batch_results, device='cuda'):\n",
    "    sam_model.eval()\n",
    "    final_predictions = []\n",
    "    for idx, sample_results in enumerate(batch_results):\n",
    "        if not sample_results:\n",
    "            final_predictions.append(torch.zeros((1, 1, 1024, 1024), device=device))\n",
    "            continue\n",
    "\n",
    "        current_image_embedding = image_embeddings[idx:idx+1]\n",
    "\n",
    "        sparse_embeddings_list = []\n",
    "        dense_embeddings_list = []\n",
    "\n",
    "        for mask_info in sample_results:\n",
    "            box = torch.tensor(mask_info['bbox'], dtype=torch.float, device=device).unsqueeze(0)\n",
    "            mask = torch.from_numpy(mask_info['mask']).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=box,\n",
    "                masks=mask\n",
    "                #masks=None\n",
    "            )\n",
    "\n",
    "            sparse_embeddings_list.append(sparse_embeddings)\n",
    "            dense_embeddings_list.append(dense_embeddings)\n",
    "\n",
    "        if len(sparse_embeddings_list) == 0:\n",
    "            final_predictions.append(torch.zeros((1, 1, 1024, 1024), device=device))\n",
    "            continue\n",
    "\n",
    "        sparse_embeddings_all = torch.cat(sparse_embeddings_list, dim=0)\n",
    "        dense_embeddings_all = torch.cat(dense_embeddings_list, dim=0)\n",
    "\n",
    "        low_res_masks, _ = sam_model.mask_decoder(\n",
    "            image_embeddings=current_image_embedding,\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings_all,\n",
    "            dense_prompt_embeddings=dense_embeddings_all,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        resized_masks = F.interpolate(\n",
    "            low_res_masks,\n",
    "            size=(1024, 1024),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        merged_mask = torch.max(resized_masks, dim=0)[0]  # [1,1024,1024]\n",
    "\n",
    "        final_predictions.append(merged_mask.unsqueeze(0))  # [1,1,1024,1024]\n",
    "\n",
    "    return torch.cat(final_predictions, dim=0)  # [B,1,1024,1024]\n",
    "\n",
    "def dice_loss(preds, targets, smooth=1e-6):\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = preds.view(preds.size(0), -1)\n",
    "    targets = targets.view(targets.size(0), -1)\n",
    "\n",
    "    intersection = (preds * targets).sum(dim=1)\n",
    "    union = preds.sum(dim=1) + targets.sum(dim=1)\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    loss = 1 - dice\n",
    "    return loss.mean()\n",
    "\n",
    "def compute_loss(\n",
    "    seg_head_logits,\n",
    "    final_predictions,\n",
    "    masks,\n",
    "    loss_fn,\n",
    "    aux_classifiers=None,\n",
    "    inter_features=None,\n",
    "    selected_aux_layers=None,\n",
    "    aux_weight=0.4,\n",
    "    seg_weight=1.0,\n",
    "    sam_weight=1.0,\n",
    "    loss_weights=[1, 1]\n",
    "):\n",
    "    seg_bce = loss_fn(seg_head_logits, masks)\n",
    "    seg_dice = dice_loss(seg_head_logits, masks)\n",
    "    seg_main_loss = loss_weights[0] * seg_bce + loss_weights[1] * seg_dice\n",
    "\n",
    "    sam_bce = loss_fn(final_predictions, masks)\n",
    "    sam_dice = dice_loss(final_predictions, masks)\n",
    "    sam_main_loss = loss_weights[0] * sam_bce + loss_weights[1] * sam_dice\n",
    "#     sam_main_loss = sam_dice\n",
    "\n",
    "    total_aux_loss = torch.tensor(0.0, device=seg_head_logits.device)\n",
    "    if aux_classifiers is not None and inter_features is not None and selected_aux_layers is not None:\n",
    "        aux_losses = []\n",
    "        for idx, aux_cls in zip(selected_aux_layers, aux_classifiers):\n",
    "            feature_idx = idx - 1\n",
    "            if feature_idx < len(inter_features):\n",
    "                aux_feat = inter_features[feature_idx]\n",
    "                aux_logits = aux_cls(aux_feat)\n",
    "                loss_aux = loss_fn(aux_logits, masks)\n",
    "                aux_losses.append(loss_aux)\n",
    "            else:\n",
    "                logging.warning(f\"inter_features does not have index {feature_idx}\")\n",
    "\n",
    "        if aux_losses:\n",
    "            aux_loss_mean = torch.mean(torch.stack(aux_losses))\n",
    "            total_aux_loss = aux_loss_mean * aux_weight\n",
    "\n",
    "    total_loss = seg_weight * seg_main_loss + sam_weight * sam_main_loss + total_aux_loss\n",
    "\n",
    "    return (\n",
    "        total_loss,\n",
    "        seg_main_loss.item(),\n",
    "        sam_main_loss.item(),\n",
    "        total_aux_loss.item()\n",
    "    )\n",
    "\n",
    "def initialize_metrics():\n",
    "    return {\n",
    "        'tp': 0,\n",
    "        'fp': 0,\n",
    "        'fn': 0,\n",
    "        'intersection': 0,\n",
    "        'union': 0\n",
    "    }\n",
    "\n",
    "def accumulate_metrics(preds, targets, global_metrics, threshold=0.5):\n",
    "    preds_binary = (preds > threshold).astype(np.uint8)\n",
    "    targets_binary = (targets > threshold).astype(np.uint8)\n",
    "\n",
    "    tp = np.logical_and(preds_binary == 1, targets_binary == 1).sum()\n",
    "    fp = np.logical_and(preds_binary == 1, targets_binary == 0).sum()\n",
    "    fn = np.logical_and(preds_binary == 0, targets_binary == 1).sum()\n",
    "\n",
    "    intersection = tp\n",
    "    union = np.logical_or(preds_binary, targets_binary).sum()\n",
    "\n",
    "    global_metrics['tp'] += tp\n",
    "    global_metrics['fp'] += fp\n",
    "    global_metrics['fn'] += fn\n",
    "    global_metrics['intersection'] += intersection\n",
    "    global_metrics['union'] += union\n",
    "\n",
    "class AuxiliaryClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=1):\n",
    "        super(AuxiliaryClassifier, self).__init__()\n",
    "        self.aux_conv1 = nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.aux_bn1 = nn.BatchNorm2d(256)\n",
    "        self.aux_relu1 = nn.ReLU(inplace=True)\n",
    "        self.aux_conv2 = nn.Conv2d(256, num_classes, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.aux_conv1(x)\n",
    "        x = self.aux_bn1(x)\n",
    "        x = self.aux_relu1(x)\n",
    "        x = self.aux_conv2(x)\n",
    "        x = F.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, intermediate_channels, out_channels=1, align_corners=False):\n",
    "        super(SegmentationHead, self).__init__()\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "        self.mla_branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1024, 512, kernel_size=3, padding=1, stride=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(512, 256, kernel_size=3, padding=1, stride=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ) for _ in range(4)\n",
    "        ])\n",
    "\n",
    "        self.mla_image_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.mla_classifier_branch = nn.Sequential(\n",
    "            nn.Conv2d(256 * 5, intermediate_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(intermediate_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(intermediate_channels, out_channels, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_embedding, inter_features):\n",
    "        if inter_features is None:\n",
    "            raise ValueError(\"inter_features must be provided for MLA strategy\")\n",
    "        if len(inter_features) < 24:\n",
    "            raise ValueError(f\"Expected at least 24 inter_features for MLA strategy, but got {len(inter_features)}\")\n",
    "\n",
    "        selected_features = [inter_features[i] for i in [5, 11, 17, 23]]\n",
    "        selected_features = [feat.permute(0, 3, 1, 2) for feat in selected_features]\n",
    "\n",
    "        processed_features = []\n",
    "        for i, feat in enumerate(selected_features):\n",
    "            branch = self.mla_branches[i]\n",
    "            x_feat = branch(feat)\n",
    "            x_feat = F.interpolate(x_feat, scale_factor=4, mode='bilinear', align_corners=self.align_corners)\n",
    "            processed_features.append(x_feat)\n",
    "\n",
    "        img_feat = self.mla_image_branch(image_embedding)\n",
    "        img_feat = F.interpolate(img_feat, scale_factor=4, mode='bilinear', align_corners=self.align_corners)\n",
    "        processed_features.append(img_feat)\n",
    "\n",
    "        aggregated = torch.cat(processed_features, dim=1)\n",
    "        x = self.mla_classifier_branch(aggregated)\n",
    "        x = F.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=self.align_corners)\n",
    "\n",
    "        return x \n",
    "\n",
    "# class SegmentationHead(nn.Module):\n",
    "#     def __init__(self, in_channels, intermediate_channels, out_channels=1, align_corners=False):\n",
    "#         super(SegmentationHead, self).__init__()\n",
    "#         self.align_corners = align_corners\n",
    "\n",
    "#         # Image branch: Processes the image embedding\n",
    "#         self.image_branch = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         # Classifier branch: Maps the processed image feature to the desired output channels\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Conv2d(256, intermediate_channels, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(intermediate_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(intermediate_channels, out_channels, kernel_size=1, stride=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, image_embedding, inter_features=None):\n",
    "#         \"\"\"\n",
    "#         Forward pass for the segmentation head.\n",
    "\n",
    "#         Args:\n",
    "#             image_embedding (torch.Tensor): The embedding of the input image. Shape: (N, C, H, W)\n",
    "#             inter_features (Optional[torch.Tensor]): Not used in this simplified version.\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: The segmentation map. Shape: (N, out_channels, 1024, 1024)\n",
    "#         \"\"\"\n",
    "#         # Process the image embedding through the image branch\n",
    "#         img_feat = self.image_branch(image_embedding)\n",
    "        \n",
    "#         # Upsample the image feature by a factor of 4\n",
    "#         img_feat = F.interpolate(img_feat, scale_factor=4, mode='bilinear', align_corners=self.align_corners)\n",
    "        \n",
    "#         # Pass the upsampled feature through the classifier to get the segmentation map\n",
    "#         x = self.classifier(img_feat)\n",
    "        \n",
    "#         # Upsample the segmentation map to the desired output size (1024x1024)\n",
    "#         x = F.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=self.align_corners)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "\n",
    "# 初始化分割头模型\n",
    "model = SegmentationHead(\n",
    "    in_channels=256,\n",
    "    intermediate_channels=256,\n",
    "    out_channels=CONFIG['num_classes'],\n",
    "    align_corners=False\n",
    ")\n",
    "model.to(CONFIG['device'])\n",
    "\n",
    "# 初始化辅助分类器\n",
    "selected_aux_layers = [5, 11, 17, 23]\n",
    "aux_classifiers = nn.ModuleList([\n",
    "    AuxiliaryClassifier(in_channels=1024, num_classes=CONFIG['num_classes']).to(CONFIG['device'])\n",
    "    for _ in selected_aux_layers\n",
    "])\n",
    "\n",
    "# 设置参数的可训练性\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for aux in aux_classifiers:\n",
    "    for param in aux.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in lora_sam_model.sam.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for layer in lora_sam_model.A_weights + lora_sam_model.B_weights:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# 收集所有可训练参数\n",
    "lora_trainable_params = list(filter(lambda p: p.requires_grad, lora_sam_model.parameters()))\n",
    "model_trainable_params = list(model.parameters())\n",
    "aux_trainable_params = list(aux_classifiers.parameters())\n",
    "trainable_params = lora_trainable_params + model_trainable_params + aux_trainable_params\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainable_params,\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    betas=CONFIG['betas'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = CONFIG['num_epochs']\n",
    "best_composite_score = float('-inf')\n",
    "best_epoch = 0\n",
    "\n",
    "weights = CONFIG['metric_weights']\n",
    "num_classes = CONFIG['num_classes']\n",
    "AUX_WEIGHT = CONFIG['aux_weight']\n",
    "\n",
    "warmup_epochs = 3\n",
    "min_lr_factor = 0.01\n",
    "\n",
    "# 学习率调度器\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        return float((epoch + 1) / warmup_epochs)\n",
    "    else:\n",
    "        cosine_decay = 0.5 * (1 + math.cos((epoch - warmup_epochs) * math.pi / (num_epochs - warmup_epochs)))\n",
    "        return float(min_lr_factor + (1 - min_lr_factor) * cosine_decay)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lora_sam_model.train()\n",
    "    model.train()\n",
    "    aux_classifiers.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\"):\n",
    "        images = images.to(CONFIG['device'])\n",
    "        masks = masks.to(CONFIG['device']).unsqueeze(1)\n",
    "\n",
    "        if images.dim() != 4 or masks.dim() != 4:\n",
    "            logging.error(f\"Invalid input dimensions: images {images.shape}, masks {masks.shape}\")\n",
    "            continue\n",
    "\n",
    "        image_embedding, inter_features = lora_sam_model.sam.image_encoder.forward_inter(images)\n",
    "        seg_head_logits = model(image_embedding, inter_features)\n",
    "\n",
    "        prompts = process_class_logits(seg_head_logits)\n",
    "        with torch.no_grad():\n",
    "            final_predictions = predict_masks_batch(lora_sam_model.sam, image_embedding, prompts, CONFIG['device'])\n",
    "\n",
    "        loss, seg_loss_val, sam_loss_val, aux_loss_val = compute_loss(\n",
    "            seg_head_logits=seg_head_logits,\n",
    "            final_predictions=final_predictions,\n",
    "            masks=masks,\n",
    "            loss_fn=loss_fn,\n",
    "            aux_classifiers=aux_classifiers,\n",
    "            inter_features=inter_features,\n",
    "            selected_aux_layers=selected_aux_layers,\n",
    "            aux_weight=AUX_WEIGHT,\n",
    "            seg_weight=1.0,\n",
    "            sam_weight=1.0,\n",
    "            loss_weights=[1,1]\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "    # 验证阶段\n",
    "    lora_sam_model.eval()\n",
    "    model.eval()\n",
    "    aux_classifiers.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    num_val_batches = 0\n",
    "    global_metrics_val = initialize_metrics()       # 最终预测指标\n",
    "    global_metrics_val_seg = initialize_metrics()   # 分割头输出指标\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Validation]\"):\n",
    "            images = images.to(CONFIG['device'])\n",
    "            masks = masks.to(CONFIG['device']).unsqueeze(1)\n",
    "\n",
    "            if images.dim() != 4 or masks.dim() != 4:\n",
    "                logging.error(f\"Invalid input dimensions: images {images.shape}, masks {masks.shape}\")\n",
    "                continue\n",
    "\n",
    "            image_embedding, inter_features = lora_sam_model.sam.image_encoder.forward_inter(images)\n",
    "            seg_head_logits = model(image_embedding, inter_features)\n",
    "\n",
    "            prompts = process_class_logits(seg_head_logits)\n",
    "            final_predictions = predict_masks_batch(lora_sam_model.sam, image_embedding, prompts, CONFIG['device'])\n",
    "\n",
    "            loss, seg_loss_val, sam_loss_val, _ = compute_loss(\n",
    "                seg_head_logits=seg_head_logits,\n",
    "                final_predictions=final_predictions,\n",
    "                masks=masks,\n",
    "                loss_fn=loss_fn,\n",
    "                aux_classifiers=None,\n",
    "                inter_features=None,\n",
    "                selected_aux_layers=None,\n",
    "                aux_weight=0.0,\n",
    "                seg_weight=1.0,\n",
    "                sam_weight=1.0,\n",
    "                loss_weights=[1,1]\n",
    "            )\n",
    "            val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "\n",
    "            # 计算分割头输出指标\n",
    "            preds_seg = torch.sigmoid(seg_head_logits).cpu().numpy()  # 分割头输出的预测\n",
    "            preds_final = torch.sigmoid(final_predictions).cpu().numpy()  # 最终预测结果\n",
    "            masks_np = masks.cpu().numpy()\n",
    "\n",
    "            for p_seg, p_final, m_gt in zip(preds_seg, preds_final, masks_np):\n",
    "                # 分割头输出指标\n",
    "                accumulate_metrics(p_seg[0], m_gt, global_metrics_val_seg)\n",
    "                # 最终预测指标\n",
    "                accumulate_metrics(p_final[0], m_gt, global_metrics_val)\n",
    "\n",
    "    # 计算评价指标（分割头输出）\n",
    "    tp_seg = global_metrics_val_seg['tp']\n",
    "    fp_seg = global_metrics_val_seg['fp']\n",
    "    fn_seg = global_metrics_val_seg['fn']\n",
    "    intersection_seg = global_metrics_val_seg['intersection']\n",
    "    union_seg = global_metrics_val_seg['union']\n",
    "\n",
    "    iou_seg = intersection_seg / (union_seg + 1e-6)\n",
    "    precision_seg = tp_seg / (tp_seg + fp_seg + 1e-6)\n",
    "    recall_seg = tp_seg / (tp_seg + fn_seg + 1e-6)\n",
    "    f1_seg = (2 * precision_seg * recall_seg) / (precision_seg + recall_seg + 1e-6)\n",
    "\n",
    "    # 计算评价指标（最终预测）\n",
    "    tp = global_metrics_val['tp']\n",
    "    fp = global_metrics_val['fp']\n",
    "    fn = global_metrics_val['fn']\n",
    "    intersection = global_metrics_val['intersection']\n",
    "    union = global_metrics_val['union']\n",
    "\n",
    "    iou = intersection / (union + 1e-6)\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = (2 * precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "    avg_iou_seg = iou_seg\n",
    "    avg_precision_seg = precision_seg\n",
    "    avg_recall_seg = recall_seg\n",
    "    avg_f1_seg = f1_seg\n",
    "\n",
    "    avg_iou = iou\n",
    "    avg_precision = precision\n",
    "    avg_recall = recall\n",
    "    avg_f1 = f1\n",
    "\n",
    "    composite_score = (\n",
    "        avg_iou * weights['iou'] +\n",
    "        avg_f1 * weights['f1'] +\n",
    "        avg_precision * weights['precision'] +\n",
    "        avg_recall * weights['recall']\n",
    "    )\n",
    "\n",
    "    avg_val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "\n",
    "    log_message = (\n",
    "        f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "        f\"(SegHead) IoU: {avg_iou_seg:.4f}, F1: {avg_f1_seg:.4f}, Precision: {avg_precision_seg:.4f}, Recall: {avg_recall_seg:.4f}, \"\n",
    "        f\"(Final) IoU: {avg_iou:.4f}, F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, \"\n",
    "        f\"Composite Score: {composite_score:.4f}, \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "    )\n",
    "    logging.info(log_message)\n",
    "    print(log_message)\n",
    "\n",
    "    if composite_score > best_composite_score:\n",
    "        best_composite_score = composite_score\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "        strategy = 'MLA'\n",
    "        lora_path = os.path.join(save_dir, f\"{CONFIG['save_prefix']}_lora_{strategy}.safetensors\")\n",
    "        checkpoint_path = os.path.join(save_dir, f\"{CONFIG['save_prefix']}_{strategy}.pth\")\n",
    "\n",
    "        if hasattr(lora_sam_model, 'save_lora_parameters'):\n",
    "            lora_sam_model.save_lora_parameters(lora_path)\n",
    "        else:\n",
    "            logging.warning(\"lora_sam_model 没有定义 save_lora_parameters 方法。\")\n",
    "\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        save_message = (\n",
    "            f\"Best model saved at epoch {best_epoch} with Composite Score {best_composite_score:.4f} using {strategy} strategy\"\n",
    "        )\n",
    "        logging.info(save_message)\n",
    "        print(save_message)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "logging.info(\"训练完成\")\n",
    "print(\"训练完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc77db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import math\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import cv2\n",
    "# import logging\n",
    "# from segment_anything import sam_model_registry\n",
    "# from tqdm import tqdm\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "# # Removed LoRA import\n",
    "# # from lora import LoRA_sam\n",
    "# from types import MethodType\n",
    "\n",
    "# # 设置随机种子\n",
    "# def set_seed(seed):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # 配置字典\n",
    "# CONFIG = {\n",
    "#     'dataset_name': 'CTS-Pore',  # 仅需设置数据集名称\n",
    "#     'data_base_dir': 'datasets',  # 数据集的基目录\n",
    "#     'log_dir_base': 'logs',        # 日志的基目录\n",
    "#     'log_file': 'best_model_metrics.log',\n",
    "#     'save_dir_base': 'logs',       # 模型保存的基目录\n",
    "#     'save_prefix': 'best_model',\n",
    "#     'model_type': 'vit_l',\n",
    "#     'checkpoint': 'weights/sam_vit_l_0b3195.pth',\n",
    "#     'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "#     'num_epochs': 100,\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'betas': (0.9, 0.999),\n",
    "#     'weight_decay': 1e-4,\n",
    "#     'metric_weights': {\n",
    "#         'iou': 0.25,\n",
    "#         'f1': 0.25,\n",
    "#         'precision': 0.25,\n",
    "#         'recall': 0.25\n",
    "#     },\n",
    "#     'aux_weight': 0.4,\n",
    "#     'num_classes': 1,\n",
    "#     # Removed 'rank' as it's specific to LoRA\n",
    "#     'batch_size': 2\n",
    "# }\n",
    "\n",
    "# # 更新随机种子\n",
    "# set_seed(42)  # 42 是任意选择的种子值\n",
    "\n",
    "# # 基于 dataset_name 构建路径\n",
    "# image_dir = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'images')\n",
    "# mask_dir = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'masks')\n",
    "# train_txt = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'train.txt')\n",
    "# val_txt = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'val.txt')\n",
    "\n",
    "# # 定义能够返回中间层的 forward 方法\n",
    "# def forward_inter(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#     x = self.patch_embed(x)\n",
    "#     if self.pos_embed is not None:\n",
    "#         x = x + self.pos_embed\n",
    "#     inter_features = []\n",
    "#     for blk in self.blocks:\n",
    "#         x = blk(x)\n",
    "#         inter_features.append(x)\n",
    "\n",
    "#     x = self.neck(x.permute(0, 3, 1, 2))\n",
    "#     return x, inter_features\n",
    "\n",
    "# # 更新日志和保存目录\n",
    "# log_dir = os.path.join(CONFIG['log_dir_base'], CONFIG['dataset_name'])\n",
    "# save_dir = os.path.join(CONFIG['save_dir_base'], CONFIG['dataset_name'])\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # 配置日志\n",
    "# logging.basicConfig(\n",
    "#     filename=os.path.join(log_dir, CONFIG['log_file']),\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "# )\n",
    "\n",
    "# # 加载 SAM 模型\n",
    "# sam_model = sam_model_registry[CONFIG['model_type']](checkpoint=CONFIG['checkpoint'])\n",
    "# sam_model.image_encoder.forward_inter = MethodType(forward_inter, sam_model.image_encoder)\n",
    "# sam_model.to(CONFIG['device'])\n",
    "\n",
    "# # 冻结 SAM 模型参数\n",
    "# for param in sam_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # 读取训练集和验证集列表\n",
    "# def read_split_files(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         file_names = f.read().strip().split('\\n')\n",
    "#     return file_names\n",
    "\n",
    "# # 数据集加载\n",
    "# class SegmentationDataset(Dataset):\n",
    "#     def __init__(self, image_dir, mask_dir, sam_model, file_list, mask_size=(1024, 1024), device='cpu'):\n",
    "#         self.image_dir = image_dir\n",
    "#         self.mask_dir = mask_dir\n",
    "#         self.sam_model = sam_model\n",
    "#         self.mask_size = mask_size\n",
    "#         self.device = device\n",
    "#         self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png') and f.replace('.png', '') in file_list]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image_file = self.image_files[idx]\n",
    "#         image_path = os.path.join(self.image_dir, image_file)\n",
    "#         image = cv2.imread(image_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#         mask_file = image_file\n",
    "#         mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "#         mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "#         mask = cv2.resize(mask, self.mask_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#         input_image_torch = torch.as_tensor(image, dtype=torch.float32).to(self.device)\n",
    "#         input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()  # [C, H, W]\n",
    "\n",
    "#         input_image = self.sam_model.preprocess(input_image_torch.to(self.device))\n",
    "\n",
    "#         mask = torch.as_tensor(mask, dtype=torch.float32).to(self.device)  # 单通道浮点数\n",
    "\n",
    "#         return input_image, mask\n",
    "\n",
    "# # 读取文件列表\n",
    "# train_files = read_split_files(train_txt)\n",
    "# val_files = read_split_files(val_txt)\n",
    "\n",
    "# # 创建数据集和数据加载器\n",
    "# train_dataset = SegmentationDataset(\n",
    "#     image_dir=image_dir,\n",
    "#     mask_dir=mask_dir,\n",
    "#     sam_model=sam_model,\n",
    "#     file_list=train_files,\n",
    "#     device=CONFIG['device']\n",
    "# )\n",
    "\n",
    "# val_dataset = SegmentationDataset(\n",
    "#     image_dir=image_dir,\n",
    "#     mask_dir=mask_dir,\n",
    "#     sam_model=sam_model,\n",
    "#     file_list=val_files,\n",
    "#     device=CONFIG['device']\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=CONFIG['batch_size'],\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=CONFIG['batch_size'],\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# def process_class_logits(class_logits):\n",
    "#     probs = torch.sigmoid(class_logits)\n",
    "#     binary_masks = (probs > 0.5).cpu().numpy().astype(np.uint8)  # [B,1,1024,1024]\n",
    "#     batch_results = []\n",
    "#     for batch_idx in range(binary_masks.shape[0]):\n",
    "#         current_mask = binary_masks[batch_idx, 0, :, :]\n",
    "#         num_labels, labels = cv2.connectedComponents(current_mask)\n",
    "#         sample_results = []\n",
    "#         for label in range(1, num_labels):\n",
    "#             current_component = (labels == label).astype(np.uint8)\n",
    "#             y_coords, x_coords = np.nonzero(current_component)\n",
    "\n",
    "#             min_x, max_x = np.min(x_coords), np.max(x_coords)\n",
    "#             min_y, max_y = np.min(y_coords), np.max(y_coords)\n",
    "\n",
    "#             mask = np.zeros_like(current_component, dtype=np.uint8)\n",
    "#             mask[min_y:max_y+1, min_x:max_x+1] = current_component[min_y:max_y+1, min_x:max_x+1]\n",
    "\n",
    "#             mask_resized = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#             sample_results.append({\n",
    "#                 'bbox': [min_x, min_y, max_x, max_y],\n",
    "#                 'mask': mask_resized\n",
    "#             })\n",
    "#         batch_results.append(sample_results)\n",
    "#     return batch_results\n",
    "\n",
    "# def predict_masks_batch(sam_model, image_embeddings, batch_results, device='cuda'):\n",
    "#     sam_model.eval()\n",
    "#     final_predictions = []\n",
    "#     for idx, sample_results in enumerate(batch_results):\n",
    "#         if not sample_results:\n",
    "#             final_predictions.append(torch.zeros((1, 1, 1024, 1024), device=device))\n",
    "#             continue\n",
    "\n",
    "#         current_image_embedding = image_embeddings[idx:idx+1]\n",
    "\n",
    "#         sparse_embeddings_list = []\n",
    "#         dense_embeddings_list = []\n",
    "\n",
    "#         for mask_info in sample_results:\n",
    "#             box = torch.tensor(mask_info['bbox'], dtype=torch.float, device=device).unsqueeze(0)\n",
    "#             mask = torch.from_numpy(mask_info['mask']).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "#             sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "#                 points=None,\n",
    "#                 boxes=box,\n",
    "#                 masks=mask\n",
    "#                 #masks=None\n",
    "#             )\n",
    "\n",
    "#             sparse_embeddings_list.append(sparse_embeddings)\n",
    "#             dense_embeddings_list.append(dense_embeddings)\n",
    "\n",
    "#         if len(sparse_embeddings_list) == 0:\n",
    "#             final_predictions.append(torch.zeros((1, 1, 1024, 1024), device=device))\n",
    "#             continue\n",
    "\n",
    "#         sparse_embeddings_all = torch.cat(sparse_embeddings_list, dim=0)\n",
    "#         dense_embeddings_all = torch.cat(dense_embeddings_list, dim=0)\n",
    "\n",
    "#         low_res_masks, _ = sam_model.mask_decoder(\n",
    "#             image_embeddings=current_image_embedding,\n",
    "#             image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "#             sparse_prompt_embeddings=sparse_embeddings_all,\n",
    "#             dense_prompt_embeddings=dense_embeddings_all,\n",
    "#             multimask_output=False,\n",
    "#         )\n",
    "\n",
    "#         resized_masks = F.interpolate(\n",
    "#             low_res_masks,\n",
    "#             size=(1024, 1024),\n",
    "#             mode='bilinear',\n",
    "#             align_corners=False\n",
    "#         )\n",
    "\n",
    "#         merged_mask = torch.max(resized_masks, dim=0)[0]  # [1,1024,1024]\n",
    "\n",
    "#         final_predictions.append(merged_mask.unsqueeze(0))  # [1,1,1024,1024]\n",
    "\n",
    "#     return torch.cat(final_predictions, dim=0)  # [B,1,1024,1024]\n",
    "\n",
    "# def dice_loss(preds, targets, smooth=1e-6):\n",
    "#     preds = torch.sigmoid(preds)\n",
    "#     preds = preds.view(preds.size(0), -1)\n",
    "#     targets = targets.view(targets.size(0), -1)\n",
    "\n",
    "#     intersection = (preds * targets).sum(dim=1)\n",
    "#     union = preds.sum(dim=1) + targets.sum(dim=1)\n",
    "\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)\n",
    "#     loss = 1 - dice\n",
    "#     return loss.mean()\n",
    "\n",
    "# def compute_loss(\n",
    "#     seg_head_logits,\n",
    "#     final_predictions,\n",
    "#     masks,\n",
    "#     loss_fn,\n",
    "#     aux_classifiers=None,\n",
    "#     inter_features=None,\n",
    "#     selected_aux_layers=None,\n",
    "#     aux_weight=0.4,\n",
    "#     seg_weight=1.0,\n",
    "#     sam_weight=1.0,\n",
    "#     loss_weights=[1, 1]\n",
    "# ):\n",
    "#     seg_bce = loss_fn(seg_head_logits, masks)\n",
    "#     seg_dice = dice_loss(seg_head_logits, masks)\n",
    "#     seg_main_loss = loss_weights[0] * seg_bce + loss_weights[1] * seg_dice\n",
    "\n",
    "#     sam_bce = loss_fn(final_predictions, masks)\n",
    "#     sam_dice = dice_loss(final_predictions, masks)\n",
    "#     sam_main_loss = loss_weights[0] * sam_bce + loss_weights[1] * sam_dice\n",
    "#     # sam_main_loss = sam_dice\n",
    "\n",
    "#     total_aux_loss = torch.tensor(0.0, device=seg_head_logits.device)\n",
    "#     if aux_classifiers is not None and inter_features is not None and selected_aux_layers is not None:\n",
    "#         aux_losses = []\n",
    "#         for idx, aux_cls in zip(selected_aux_layers, aux_classifiers):\n",
    "#             feature_idx = idx - 1\n",
    "#             if feature_idx < len(inter_features):\n",
    "#                 aux_feat = inter_features[feature_idx]\n",
    "#                 aux_logits = aux_cls(aux_feat)\n",
    "#                 loss_aux = loss_fn(aux_logits, masks)\n",
    "#                 aux_losses.append(loss_aux)\n",
    "#             else:\n",
    "#                 logging.warning(f\"inter_features does not have index {feature_idx}\")\n",
    "\n",
    "#         if aux_losses:\n",
    "#             aux_loss_mean = torch.mean(torch.stack(aux_losses))\n",
    "#             total_aux_loss = aux_loss_mean * aux_weight\n",
    "\n",
    "#     total_loss = seg_weight * seg_main_loss + sam_weight * sam_main_loss + total_aux_loss\n",
    "\n",
    "#     return (\n",
    "#         total_loss,\n",
    "#         seg_main_loss.item(),\n",
    "#         sam_main_loss.item(),\n",
    "#         total_aux_loss.item()\n",
    "#     )\n",
    "\n",
    "# def initialize_metrics():\n",
    "#     return {\n",
    "#         'tp': 0,\n",
    "#         'fp': 0,\n",
    "#         'fn': 0,\n",
    "#         'intersection': 0,\n",
    "#         'union': 0\n",
    "#     }\n",
    "\n",
    "# def accumulate_metrics(preds, targets, global_metrics, threshold=0.5):\n",
    "#     preds_binary = (preds > threshold).astype(np.uint8)\n",
    "#     targets_binary = (targets > threshold).astype(np.uint8)\n",
    "\n",
    "#     tp = np.logical_and(preds_binary == 1, targets_binary == 1).sum()\n",
    "#     fp = np.logical_and(preds_binary == 1, targets_binary == 0).sum()\n",
    "#     fn = np.logical_and(preds_binary == 0, targets_binary == 1).sum()\n",
    "\n",
    "#     intersection = tp\n",
    "#     union = np.logical_or(preds_binary, targets_binary).sum()\n",
    "\n",
    "#     global_metrics['tp'] += tp\n",
    "#     global_metrics['fp'] += fp\n",
    "#     global_metrics['fn'] += fn\n",
    "#     global_metrics['intersection'] += intersection\n",
    "#     global_metrics['union'] += union\n",
    "\n",
    "# class AuxiliaryClassifier(nn.Module):\n",
    "#     def __init__(self, in_channels, num_classes=1):\n",
    "#         super(AuxiliaryClassifier, self).__init__()\n",
    "#         self.aux_conv1 = nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1)\n",
    "#         self.aux_bn1 = nn.BatchNorm2d(256)\n",
    "#         self.aux_relu1 = nn.ReLU(inplace=True)\n",
    "#         self.aux_conv2 = nn.Conv2d(256, num_classes, kernel_size=1, stride=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 3, 1, 2)\n",
    "#         x = self.aux_conv1(x)\n",
    "#         x = self.aux_bn1(x)\n",
    "#         x = self.aux_relu1(x)\n",
    "#         x = self.aux_conv2(x)\n",
    "#         x = F.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
    "#         return x\n",
    "\n",
    "# class SegmentationHead(nn.Module):\n",
    "#     def __init__(self, in_channels, intermediate_channels, out_channels=1, align_corners=False):\n",
    "#         super(SegmentationHead, self).__init__()\n",
    "#         self.align_corners = align_corners\n",
    "\n",
    "#         self.mla_branches = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(1024, 512, kernel_size=3, padding=1, stride=1),\n",
    "#                 nn.BatchNorm2d(512),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Conv2d(512, 256, kernel_size=3, padding=1, stride=1),\n",
    "#                 nn.BatchNorm2d(256),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ) for _ in range(4)\n",
    "#         ])\n",
    "\n",
    "#         self.mla_image_branch = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#         self.mla_classifier_branch = nn.Sequential(\n",
    "#             nn.Conv2d(256 * 5, intermediate_channels, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(intermediate_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(intermediate_channels, out_channels, kernel_size=1, stride=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, image_embedding, inter_features):\n",
    "#         if inter_features is None:\n",
    "#             raise ValueError(\"inter_features must be provided for MLA strategy\")\n",
    "#         if len(inter_features) < 24:\n",
    "#             raise ValueError(f\"Expected at least 24 inter_features for MLA strategy, but got {len(inter_features)}\")\n",
    "\n",
    "#         selected_features = [inter_features[i] for i in [5, 11, 17, 23]]\n",
    "#         selected_features = [feat.permute(0, 3, 1, 2) for feat in selected_features]\n",
    "\n",
    "#         processed_features = []\n",
    "#         for i, feat in enumerate(selected_features):\n",
    "#             branch = self.mla_branches[i]\n",
    "#             x_feat = branch(feat)\n",
    "#             x_feat = F.interpolate(x_feat, scale_factor=4, mode='bilinear', align_corners=self.align_corners)\n",
    "#             processed_features.append(x_feat)\n",
    "\n",
    "#         img_feat = self.mla_image_branch(image_embedding)\n",
    "#         img_feat = F.interpolate(img_feat, scale_factor=4, mode='bilinear', align_corners=self.align_corners)\n",
    "#         processed_features.append(img_feat)\n",
    "\n",
    "#         aggregated = torch.cat(processed_features, dim=1)\n",
    "#         x = self.mla_classifier_branch(aggregated)\n",
    "#         x = F.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=self.align_corners)\n",
    "\n",
    "#         return x \n",
    "\n",
    "# # 初始化分割头模型\n",
    "# model = SegmentationHead(\n",
    "#     in_channels=256,\n",
    "#     intermediate_channels=256,\n",
    "#     out_channels=CONFIG['num_classes'],\n",
    "#     align_corners=False\n",
    "# )\n",
    "# model.to(CONFIG['device'])\n",
    "\n",
    "# # 初始化辅助分类器\n",
    "# selected_aux_layers = [5, 11, 17, 23]\n",
    "# aux_classifiers = nn.ModuleList([\n",
    "#     AuxiliaryClassifier(in_channels=1024, num_classes=CONFIG['num_classes']).to(CONFIG['device'])\n",
    "#     for _ in selected_aux_layers\n",
    "# ])\n",
    "\n",
    "# # 设置参数的可训练性\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# for aux in aux_classifiers:\n",
    "#     for param in aux.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # 确保 SAM 模型参数不参与训练\n",
    "# # (Already done by setting requires_grad=False above)\n",
    "\n",
    "# # 收集所有可训练参数 (Only model and auxiliary classifiers)\n",
    "# model_trainable_params = list(model.parameters())\n",
    "# aux_trainable_params = list(aux_classifiers.parameters())\n",
    "# trainable_params = model_trainable_params + aux_trainable_params\n",
    "\n",
    "# # 初始化优化器\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     trainable_params,\n",
    "#     lr=CONFIG['learning_rate'],\n",
    "#     betas=CONFIG['betas'],\n",
    "#     weight_decay=CONFIG['weight_decay']\n",
    "# )\n",
    "\n",
    "# # 损失函数\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# num_epochs = CONFIG['num_epochs']\n",
    "# best_composite_score = float('-inf')\n",
    "# best_epoch = 0\n",
    "\n",
    "# weights = CONFIG['metric_weights']\n",
    "# num_classes = CONFIG['num_classes']\n",
    "# AUX_WEIGHT = CONFIG['aux_weight']\n",
    "\n",
    "# warmup_epochs = 3\n",
    "# min_lr_factor = 0.01\n",
    "\n",
    "# # 学习率调度器\n",
    "# def lr_lambda(epoch):\n",
    "#     if epoch < warmup_epochs:\n",
    "#         return float((epoch + 1) / warmup_epochs)\n",
    "#     else:\n",
    "#         cosine_decay = 0.5 * (1 + math.cos((epoch - warmup_epochs) * math.pi / (num_epochs - warmup_epochs)))\n",
    "#         return float(min_lr_factor + (1 - min_lr_factor) * cosine_decay)\n",
    "\n",
    "# scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     aux_classifiers.train()\n",
    "\n",
    "#     total_loss = 0\n",
    "#     num_batches = 0\n",
    "\n",
    "#     for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\"):\n",
    "#         images = images.to(CONFIG['device'])\n",
    "#         masks = masks.to(CONFIG['device']).unsqueeze(1)\n",
    "\n",
    "#         if images.dim() != 4 or masks.dim() != 4:\n",
    "#             logging.error(f\"Invalid input dimensions: images {images.shape}, masks {masks.shape}\")\n",
    "#             continue\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             image_embedding, inter_features = sam_model.image_encoder.forward_inter(images)\n",
    "#         seg_head_logits = model(image_embedding, inter_features)\n",
    "\n",
    "#         prompts = process_class_logits(seg_head_logits)\n",
    "#         final_predictions = predict_masks_batch(sam_model, image_embedding, prompts, CONFIG['device'])\n",
    "\n",
    "#         loss, seg_loss_val, sam_loss_val, aux_loss_val = compute_loss(\n",
    "#             seg_head_logits=seg_head_logits,\n",
    "#             final_predictions=final_predictions,\n",
    "#             masks=masks,\n",
    "#             loss_fn=loss_fn,\n",
    "#             aux_classifiers=aux_classifiers,\n",
    "#             inter_features=inter_features,\n",
    "#             selected_aux_layers=selected_aux_layers,\n",
    "#             aux_weight=AUX_WEIGHT,\n",
    "#             seg_weight=1.0,\n",
    "#             sam_weight=1.0,\n",
    "#             loss_weights=[1,1]\n",
    "#         )\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         num_batches += 1\n",
    "\n",
    "#     avg_train_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "#     # 验证阶段\n",
    "#     model.eval()\n",
    "#     aux_classifiers.eval()\n",
    "\n",
    "#     val_loss = 0\n",
    "#     num_val_batches = 0\n",
    "#     global_metrics_val = initialize_metrics()       # 最终预测指标\n",
    "#     global_metrics_val_seg = initialize_metrics()   # 分割头输出指标\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Validation]\"):\n",
    "#             images = images.to(CONFIG['device'])\n",
    "#             masks = masks.to(CONFIG['device']).unsqueeze(1)\n",
    "\n",
    "#             if images.dim() != 4 or masks.dim() != 4:\n",
    "#                 logging.error(f\"Invalid input dimensions: images {images.shape}, masks {masks.shape}\")\n",
    "#                 continue\n",
    "\n",
    "#             image_embedding, inter_features = sam_model.image_encoder.forward_inter(images)\n",
    "#             seg_head_logits = model(image_embedding, inter_features)\n",
    "\n",
    "#             prompts = process_class_logits(seg_head_logits)\n",
    "#             final_predictions = predict_masks_batch(sam_model, image_embedding, prompts, CONFIG['device'])\n",
    "\n",
    "#             loss, seg_loss_val, sam_loss_val, _ = compute_loss(\n",
    "#                 seg_head_logits=seg_head_logits,\n",
    "#                 final_predictions=final_predictions,\n",
    "#                 masks=masks,\n",
    "#                 loss_fn=loss_fn,\n",
    "#                 aux_classifiers=None,\n",
    "#                 inter_features=None,\n",
    "#                 selected_aux_layers=None,\n",
    "#                 aux_weight=0.0,\n",
    "#                 seg_weight=1.0,\n",
    "#                 sam_weight=1.0,\n",
    "#                 loss_weights=[1,1]\n",
    "#             )\n",
    "#             val_loss += loss.item()\n",
    "#             num_val_batches += 1\n",
    "\n",
    "#             # 计算分割头输出指标\n",
    "#             preds_seg = torch.sigmoid(seg_head_logits).cpu().numpy()  # 分割头输出的预测\n",
    "#             preds_final = torch.sigmoid(final_predictions).cpu().numpy()  # 最终预测结果\n",
    "#             masks_np = masks.cpu().numpy()\n",
    "\n",
    "#             for p_seg, p_final, m_gt in zip(preds_seg, preds_final, masks_np):\n",
    "#                 # 分割头输出指标\n",
    "#                 accumulate_metrics(p_seg[0], m_gt, global_metrics_val_seg)\n",
    "#                 # 最终预测指标\n",
    "#                 accumulate_metrics(p_final[0], m_gt, global_metrics_val)\n",
    "\n",
    "#     # 计算评价指标（分割头输出）\n",
    "#     tp_seg = global_metrics_val_seg['tp']\n",
    "#     fp_seg = global_metrics_val_seg['fp']\n",
    "#     fn_seg = global_metrics_val_seg['fn']\n",
    "#     intersection_seg = global_metrics_val_seg['intersection']\n",
    "#     union_seg = global_metrics_val_seg['union']\n",
    "\n",
    "#     iou_seg = intersection_seg / (union_seg + 1e-6)\n",
    "#     precision_seg = tp_seg / (tp_seg + fp_seg + 1e-6)\n",
    "#     recall_seg = tp_seg / (tp_seg + fn_seg + 1e-6)\n",
    "#     f1_seg = (2 * precision_seg * recall_seg) / (precision_seg + recall_seg + 1e-6)\n",
    "\n",
    "#     # 计算评价指标（最终预测）\n",
    "#     tp = global_metrics_val['tp']\n",
    "#     fp = global_metrics_val['fp']\n",
    "#     fn = global_metrics_val['fn']\n",
    "#     intersection = global_metrics_val['intersection']\n",
    "#     union = global_metrics_val['union']\n",
    "\n",
    "#     iou = intersection / (union + 1e-6)\n",
    "#     precision = tp / (tp + fp + 1e-6)\n",
    "#     recall = tp / (tp + fn + 1e-6)\n",
    "#     f1 = (2 * precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "#     avg_iou_seg = iou_seg\n",
    "#     avg_precision_seg = precision_seg\n",
    "#     avg_recall_seg = recall_seg\n",
    "#     avg_f1_seg = f1_seg\n",
    "\n",
    "#     avg_iou = iou\n",
    "#     avg_precision = precision\n",
    "#     avg_recall = recall\n",
    "#     avg_f1 = f1\n",
    "\n",
    "#     composite_score = (\n",
    "#         avg_iou * weights['iou'] +\n",
    "#         avg_f1 * weights['f1'] +\n",
    "#         avg_precision * weights['precision'] +\n",
    "#         avg_recall * weights['recall']\n",
    "#     )\n",
    "\n",
    "#     avg_val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "\n",
    "#     log_message = (\n",
    "#         f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "#         f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "#         f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "#         f\"(SegHead) IoU: {avg_iou_seg:.4f}, F1: {avg_f1_seg:.4f}, Precision: {avg_precision_seg:.4f}, Recall: {avg_recall_seg:.4f}, \"\n",
    "#         f\"(Final) IoU: {avg_iou:.4f}, F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, \"\n",
    "#         f\"Composite Score: {composite_score:.4f}, \"\n",
    "#         f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "#     )\n",
    "#     logging.info(log_message)\n",
    "#     print(log_message)\n",
    "\n",
    "#     if composite_score > best_composite_score:\n",
    "#         best_composite_score = composite_score\n",
    "#         best_epoch = epoch + 1\n",
    "\n",
    "#         strategy = 'MLA'\n",
    "#         checkpoint_path = os.path.join(save_dir, f\"{CONFIG['save_prefix']}_{strategy}.pth\")\n",
    "\n",
    "#         # 只保存分割头模型\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "#         save_message = (\n",
    "#             f\"Best model saved at epoch {best_epoch} with Composite Score {best_composite_score:.4f} using {strategy} strategy\"\n",
    "#         )\n",
    "#         logging.info(save_message)\n",
    "#         print(save_message)\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "# logging.info(\"训练完成\")\n",
    "# print(\"训练完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08649a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import math\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import cv2\n",
    "# import logging\n",
    "# from segment_anything import sam_model_registry\n",
    "# from tqdm import tqdm\n",
    "# from types import MethodType\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# # 设置随机种子（确保结果可复现）\n",
    "# def set_seed(seed):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # 配置字典\n",
    "# CONFIG = {\n",
    "#     'dataset_name': 'CTS-Pore',  # 数据集名称\n",
    "#     'data_base_dir': 'datasets',  # 数据集基目录\n",
    "#     'log_dir_base': 'logs',        # 日志基目录\n",
    "#     'log_file': 'best_model_metrics.log',\n",
    "#     'save_dir_base': 'logs',       # 模型保存基目录\n",
    "#     'save_prefix': 'best_model',\n",
    "#     'model_type': 'vit_l',\n",
    "#     'checkpoint': 'weights/sam_vit_l_0b3195.pth',\n",
    "#     'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "#     'num_epochs': 100,\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'betas': (0.9, 0.999),\n",
    "#     'weight_decay': 1e-4,\n",
    "#     'metric_weights': {\n",
    "#         'iou': 0.25,\n",
    "#         'f1': 0.25,\n",
    "#         'precision': 0.25,\n",
    "#         'recall': 0.25\n",
    "#     },\n",
    "#     'aux_weight': 0.4,\n",
    "#     'num_classes': 1,\n",
    "#     'batch_size': 2\n",
    "# }\n",
    "\n",
    "# # 更新随机种子\n",
    "# set_seed(42)  # 42 是任意选择的种子值\n",
    "\n",
    "# # 基于 dataset_name 构建路径\n",
    "# image_dir = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'images')\n",
    "# mask_dir = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'masks')\n",
    "# train_txt = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'train.txt')\n",
    "# val_txt = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'val.txt')\n",
    "\n",
    "# # 定义适配器模块\n",
    "# class Adapter(nn.Module):\n",
    "#     def __init__(self, input_dim, reduction_factor=256):\n",
    "#         super(Adapter, self).__init__()\n",
    "#         self.down_project = nn.Linear(input_dim, input_dim // reduction_factor)\n",
    "#         self.activation = nn.GELU()\n",
    "#         self.up_project = nn.Linear(input_dim // reduction_factor, input_dim)\n",
    "#         self.layer_norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "#         # 初始化权重\n",
    "#         nn.init.normal_(self.down_project.weight, std=1e-3)\n",
    "#         nn.init.normal_(self.up_project.weight, std=1e-3)\n",
    "#         nn.init.zeros_(self.down_project.bias)\n",
    "#         nn.init.zeros_(self.up_project.bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "#         x = self.layer_norm(x)\n",
    "#         x = self.down_project(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.up_project(x)\n",
    "#         return x + residual\n",
    "\n",
    "# # 修改 SAM 的 forward_inter 函数，加入适配器\n",
    "# def forward_inter(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#     x = self.patch_embed(x)\n",
    "#     if self.pos_embed is not None:\n",
    "#         x = x + self.pos_embed\n",
    "    \n",
    "#     # 创建每个 transformer 块的适配器（仅在第一次调用时）\n",
    "#     if not hasattr(self, 'adapters'):\n",
    "#         self.adapters = nn.ModuleList([\n",
    "#             Adapter(input_dim=x.shape[-1]) \n",
    "#             for _ in self.blocks\n",
    "#         ])\n",
    "        \n",
    "#         # 冻结所有参数\n",
    "#         for param in self.parameters():\n",
    "#             param.requires_grad = False\n",
    "            \n",
    "#         # 仅解冻适配器参数\n",
    "#         for adapter in self.adapters:\n",
    "#             for param in adapter.parameters():\n",
    "#                 param.requires_grad = True\n",
    "                \n",
    "#         # 如果存在分类器，解冻其参数\n",
    "#         if hasattr(self, 'classifier'):\n",
    "#             for param in self.classifier.parameters():\n",
    "#                 param.requires_grad = True\n",
    "\n",
    "#     inter_features = []\n",
    "#     for i, blk in enumerate(self.blocks):\n",
    "#         x = blk(x)\n",
    "#         # 在每个 transformer 块之后应用适配器\n",
    "#         x = self.adapters[i](x)\n",
    "#         inter_features.append(x)\n",
    "\n",
    "#     x = self.neck(x.permute(0, 3, 1, 2))\n",
    "#     return x, inter_features\n",
    "\n",
    "# # 定义分类头\n",
    "# class SegmentationHead(nn.Module):\n",
    "#     def __init__(self, in_channels, intermediate_channels, out_channels=1, align_corners=False):\n",
    "#         super(SegmentationHead, self).__init__()\n",
    "#         self.align_corners = align_corners\n",
    "\n",
    "#         self.mla_branches = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(1024, 512, kernel_size=3, padding=1, stride=1),\n",
    "#                 nn.BatchNorm2d(512),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Conv2d(512, 256, kernel_size=3, padding=1, stride=1),\n",
    "#                 nn.BatchNorm2d(256),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ) for _ in range(4)\n",
    "#         ])\n",
    "\n",
    "#         self.mla_image_branch = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#         self.mla_classifier_branch = nn.Sequential(\n",
    "#             nn.Conv2d(256 * 5, intermediate_channels, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(intermediate_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(intermediate_channels, out_channels, kernel_size=1, stride=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, image_embedding, inter_features):\n",
    "#         if inter_features is None:\n",
    "#             raise ValueError(\"inter_features must be provided for MLA strategy\")\n",
    "#         if len(inter_features) < 24:\n",
    "#             raise ValueError(f\"Expected at least 24 inter_features for MLA strategy, but got {len(inter_features)}\")\n",
    "\n",
    "#         selected_features = [inter_features[i] for i in [5, 11, 17, 23]]\n",
    "#         selected_features = [feat.permute(0, 3, 1, 2) for feat in selected_features]\n",
    "\n",
    "#         processed_features = []\n",
    "#         for i, feat in enumerate(selected_features):\n",
    "#             branch = self.mla_branches[i]\n",
    "#             x_feat = branch(feat)\n",
    "#             x_feat = F.interpolate(x_feat, scale_factor=4, mode='bilinear', align_corners=self.align_corners)\n",
    "#             processed_features.append(x_feat)\n",
    "\n",
    "#         img_feat = self.mla_image_branch(image_embedding)\n",
    "#         img_feat = F.interpolate(img_feat, scale_factor=4, mode='bilinear', align_corners=self.align_corners)\n",
    "#         processed_features.append(img_feat)\n",
    "\n",
    "#         aggregated = torch.cat(processed_features, dim=1)\n",
    "#         x = self.mla_classifier_branch(aggregated)\n",
    "#         x = F.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=self.align_corners)\n",
    "\n",
    "#         return x \n",
    "\n",
    "# # 定义辅助分类器\n",
    "# class AuxiliaryClassifier(nn.Module):\n",
    "#     def __init__(self, in_channels, num_classes=1):\n",
    "#         super(AuxiliaryClassifier, self).__init__()\n",
    "#         self.aux_conv1 = nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1)\n",
    "#         self.aux_bn1 = nn.BatchNorm2d(256)\n",
    "#         self.aux_relu1 = nn.ReLU(inplace=True)\n",
    "#         self.aux_conv2 = nn.Conv2d(256, num_classes, kernel_size=1, stride=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 3, 1, 2)  # 调整形状为 [B, C, H, W]\n",
    "#         x = self.aux_conv1(x)\n",
    "#         x = self.aux_bn1(x)\n",
    "#         x = self.aux_relu1(x)\n",
    "#         x = self.aux_conv2(x)\n",
    "#         x = F.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
    "#         return x\n",
    "\n",
    "# # 读取文件列表\n",
    "# def read_split_files(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         file_names = f.read().strip().split('\\n')\n",
    "#     return file_names\n",
    "\n",
    "# # 数据集加载\n",
    "# class SegmentationDataset(Dataset):\n",
    "#     def __init__(self, image_dir, mask_dir, sam_model, file_list, mask_size=(1024, 1024), device='cpu'):\n",
    "#         self.image_dir = image_dir\n",
    "#         self.mask_dir = mask_dir\n",
    "#         self.sam_model = sam_model\n",
    "#         self.mask_size = mask_size\n",
    "#         self.device = device\n",
    "#         self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png') and f.replace('.png', '') in file_list]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image_file = self.image_files[idx]\n",
    "#         image_path = os.path.join(self.image_dir, image_file)\n",
    "#         image = cv2.imread(image_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#         mask_file = image_file\n",
    "#         mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "#         mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "#         mask = cv2.resize(mask, self.mask_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#         input_image_torch = torch.as_tensor(image, dtype=torch.float32).to(self.device)\n",
    "#         input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()  # [C, H, W]\n",
    "\n",
    "#         input_image = self.sam_model.preprocess(input_image_torch.to(self.device))\n",
    "\n",
    "#         mask = torch.as_tensor(mask, dtype=torch.float32).to(self.device)  # 单通道浮点数\n",
    "\n",
    "#         return input_image, mask\n",
    "\n",
    "# # 处理分类器输出的掩码\n",
    "# def process_class_logits(class_logits):\n",
    "#     probs = torch.sigmoid(class_logits)\n",
    "#     binary_masks = (probs > 0.5).cpu().numpy().astype(np.uint8)  # [B,1,1024,1024]\n",
    "#     batch_results = []\n",
    "#     for batch_idx in range(binary_masks.shape[0]):\n",
    "#         current_mask = binary_masks[batch_idx, 0, :, :]\n",
    "#         num_labels, labels = cv2.connectedComponents(current_mask)\n",
    "#         sample_results = []\n",
    "#         for label in range(1, num_labels):\n",
    "#             current_component = (labels == label).astype(np.uint8)\n",
    "#             y_coords, x_coords = np.nonzero(current_component)\n",
    "\n",
    "#             min_x, max_x = np.min(x_coords), np.max(x_coords)\n",
    "#             min_y, max_y = np.min(y_coords), np.max(y_coords)\n",
    "\n",
    "#             mask = np.zeros_like(current_component, dtype=np.uint8)\n",
    "#             mask[min_y:max_y+1, min_x:max_x+1] = current_component[min_y:max_y+1, min_x:max_x+1]\n",
    "\n",
    "#             mask_resized = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#             sample_results.append({\n",
    "#                 'bbox': [min_x, min_y, max_x, max_y],\n",
    "#                 'mask': mask_resized\n",
    "#             })\n",
    "#         batch_results.append(sample_results)\n",
    "#     return batch_results\n",
    "\n",
    "# # 预测批量掩码\n",
    "# def predict_masks_batch(sam_model, image_embeddings, batch_results, device='cuda'):\n",
    "#     sam_model.eval()\n",
    "#     final_predictions = []\n",
    "#     for idx, sample_results in enumerate(batch_results):\n",
    "#         if not sample_results:\n",
    "#             final_predictions.append(torch.zeros((1, 1, 1024, 1024), device=device))\n",
    "#             continue\n",
    "\n",
    "#         current_image_embedding = image_embeddings[idx:idx+1]\n",
    "\n",
    "#         sparse_embeddings_list = []\n",
    "#         dense_embeddings_list = []\n",
    "\n",
    "#         for mask_info in sample_results:\n",
    "#             box = torch.tensor(mask_info['bbox'], dtype=torch.float, device=device).unsqueeze(0)\n",
    "#             mask = torch.from_numpy(mask_info['mask']).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "#             sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "#                 points=None,\n",
    "#                 boxes=box,\n",
    "#                 masks=mask\n",
    "#             )\n",
    "\n",
    "#             sparse_embeddings_list.append(sparse_embeddings)\n",
    "#             dense_embeddings_list.append(dense_embeddings)\n",
    "\n",
    "#         if len(sparse_embeddings_list) == 0:\n",
    "#             final_predictions.append(torch.zeros((1, 1, 1024, 1024), device=device))\n",
    "#             continue\n",
    "\n",
    "#         sparse_embeddings_all = torch.cat(sparse_embeddings_list, dim=0)\n",
    "#         dense_embeddings_all = torch.cat(dense_embeddings_list, dim=0)\n",
    "\n",
    "#         low_res_masks, _ = sam_model.mask_decoder(\n",
    "#             image_embeddings=current_image_embedding,\n",
    "#             image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "#             sparse_prompt_embeddings=sparse_embeddings_all,\n",
    "#             dense_prompt_embeddings=dense_embeddings_all,\n",
    "#             multimask_output=False,\n",
    "#         )\n",
    "\n",
    "#         resized_masks = F.interpolate(\n",
    "#             low_res_masks,\n",
    "#             size=(1024, 1024),\n",
    "#             mode='bilinear',\n",
    "#             align_corners=False\n",
    "#         )\n",
    "\n",
    "#         merged_mask = torch.max(resized_masks, dim=0)[0]  # [1,1024,1024]\n",
    "\n",
    "#         final_predictions.append(merged_mask.unsqueeze(0))  # [1,1,1024,1024]\n",
    "\n",
    "#     return torch.cat(final_predictions, dim=0)  # [B,1,1024,1024]\n",
    "\n",
    "# # Dice 损失函数\n",
    "# def dice_loss(preds, targets, smooth=1e-6):\n",
    "#     preds = torch.sigmoid(preds)\n",
    "#     preds = preds.view(preds.size(0), -1)\n",
    "#     targets = targets.view(targets.size(0), -1)\n",
    "\n",
    "#     intersection = (preds * targets).sum(dim=1)\n",
    "#     union = preds.sum(dim=1) + targets.sum(dim=1)\n",
    "\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)\n",
    "#     loss = 1 - dice\n",
    "#     return loss.mean()\n",
    "\n",
    "# # 计算综合损失\n",
    "# def compute_loss(\n",
    "#     seg_head_logits,\n",
    "#     final_predictions,\n",
    "#     masks,\n",
    "#     loss_fn,\n",
    "#     aux_classifiers=None,\n",
    "#     inter_features=None,\n",
    "#     selected_aux_layers=None,\n",
    "#     aux_weight=0.4,\n",
    "#     seg_weight=1.0,\n",
    "#     sam_weight=1.0,\n",
    "#     loss_weights=[1, 1]\n",
    "# ):\n",
    "#     seg_bce = loss_fn(seg_head_logits, masks)\n",
    "#     seg_dice = dice_loss(seg_head_logits, masks)\n",
    "#     seg_main_loss = loss_weights[0] * seg_bce + loss_weights[1] * seg_dice\n",
    "\n",
    "#     sam_bce = loss_fn(final_predictions, masks)\n",
    "#     sam_dice = dice_loss(final_predictions, masks)\n",
    "#     sam_main_loss = loss_weights[0] * sam_bce + loss_weights[1] * sam_dice\n",
    "\n",
    "#     total_aux_loss = torch.tensor(0.0, device=seg_head_logits.device)\n",
    "#     if aux_classifiers is not None and inter_features is not None and selected_aux_layers is not None:\n",
    "#         aux_losses = []\n",
    "#         for idx, aux_cls in zip(selected_aux_layers, aux_classifiers):\n",
    "#             feature_idx = idx - 1\n",
    "#             if feature_idx < len(inter_features):\n",
    "#                 aux_feat = inter_features[feature_idx]\n",
    "#                 aux_logits = aux_cls(aux_feat)\n",
    "#                 loss_aux = loss_fn(aux_logits, masks)\n",
    "#                 aux_losses.append(loss_aux)\n",
    "#             else:\n",
    "#                 logging.warning(f\"inter_features does not have index {feature_idx}\")\n",
    "\n",
    "#         if aux_losses:\n",
    "#             aux_loss_mean = torch.mean(torch.stack(aux_losses))\n",
    "#             total_aux_loss = aux_loss_mean * aux_weight\n",
    "\n",
    "#     total_loss = seg_weight * seg_main_loss + sam_weight * sam_main_loss + total_aux_loss\n",
    "\n",
    "#     return (\n",
    "#         total_loss,\n",
    "#         seg_main_loss.item(),\n",
    "#         sam_main_loss.item(),\n",
    "#         total_aux_loss.item()\n",
    "#     )\n",
    "\n",
    "# # 初始化评估指标\n",
    "# def initialize_metrics():\n",
    "#     return {\n",
    "#         'tp': 0,\n",
    "#         'fp': 0,\n",
    "#         'fn': 0,\n",
    "#         'intersection': 0,\n",
    "#         'union': 0\n",
    "#     }\n",
    "\n",
    "# # 累积评估指标\n",
    "# def accumulate_metrics(preds, targets, global_metrics, threshold=0.5):\n",
    "#     preds_binary = (preds > threshold).astype(np.uint8)\n",
    "#     targets_binary = (targets > threshold).astype(np.uint8)\n",
    "\n",
    "#     tp = np.logical_and(preds_binary == 1, targets_binary == 1).sum()\n",
    "#     fp = np.logical_and(preds_binary == 1, targets_binary == 0).sum()\n",
    "#     fn = np.logical_and(preds_binary == 0, targets_binary == 1).sum()\n",
    "\n",
    "#     intersection = tp\n",
    "#     union = np.logical_or(preds_binary, targets_binary).sum()\n",
    "\n",
    "#     global_metrics['tp'] += tp\n",
    "#     global_metrics['fp'] += fp\n",
    "#     global_metrics['fn'] += fn\n",
    "#     global_metrics['intersection'] += intersection\n",
    "#     global_metrics['union'] += union\n",
    "\n",
    "# # 初始化分割头模型\n",
    "# segmentation_head = SegmentationHead(\n",
    "#     in_channels=256,\n",
    "#     intermediate_channels=256,\n",
    "#     out_channels=CONFIG['num_classes'],\n",
    "#     align_corners=False\n",
    "# )\n",
    "# segmentation_head.to(CONFIG['device'])\n",
    "\n",
    "# # 初始化辅助分类器\n",
    "# selected_aux_layers = [5, 11, 17, 23]\n",
    "# auxiliary_classifiers = nn.ModuleList([\n",
    "#     AuxiliaryClassifier(in_channels=1024, num_classes=CONFIG['num_classes']).to(CONFIG['device'])\n",
    "#     for _ in selected_aux_layers\n",
    "# ])\n",
    "\n",
    "# # 定义 forward_inter 方法并绑定到 SAM 的 image_encoder\n",
    "# sam_model = sam_model_registry[CONFIG['model_type']](checkpoint=CONFIG['checkpoint'])\n",
    "# sam_model.image_encoder.forward_inter = MethodType(forward_inter, sam_model.image_encoder)\n",
    "# sam_model.to(CONFIG['device'])\n",
    "\n",
    "# # 手动初始化 adapters\n",
    "# # 获取输入维度\n",
    "# dummy_input = torch.zeros(1, 3, 1024, 1024).to(CONFIG['device'])\n",
    "# with torch.no_grad():\n",
    "#     x = sam_model.image_encoder.patch_embed(dummy_input)\n",
    "#     input_dim = x.shape[-1]\n",
    "\n",
    "# # 创建适配器列表\n",
    "# sam_model.image_encoder.adapters = nn.ModuleList([\n",
    "#       Adapter(\n",
    "#         input_dim=input_dim,\n",
    "#         reduction_factor=256\n",
    "#     ) \n",
    "#     for _ in sam_model.image_encoder.blocks\n",
    "# ])\n",
    "# sam_model.image_encoder.adapters.to(CONFIG['device'])\n",
    "\n",
    "# # 冻结 SAM 模型所有参数（适配器除外）\n",
    "# for param in sam_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # 确保适配器参数可训练\n",
    "# for adapter in sam_model.image_encoder.adapters:\n",
    "#     for param in adapter.parameters():\n",
    "#         param.requires_grad = True\n",
    "# # 收集所有可训练参数：适配器参数、分割头参数和辅助分类器参数\n",
    "# trainable_params = list(segmentation_head.parameters()) + list(auxiliary_classifiers.parameters()) + list(sam_model.image_encoder.adapters.parameters())\n",
    "\n",
    "# # 初始化优化器\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     trainable_params,\n",
    "#     lr=CONFIG['learning_rate'],\n",
    "#     betas=CONFIG['betas'],\n",
    "#     weight_decay=CONFIG['weight_decay']\n",
    "# )\n",
    "\n",
    "# # 损失函数\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# # 读取训练集和验证集列表\n",
    "# train_files = read_split_files(train_txt)\n",
    "# val_files = read_split_files(val_txt)\n",
    "\n",
    "# # 创建数据集和数据加载器\n",
    "# train_dataset = SegmentationDataset(\n",
    "#     image_dir=image_dir,\n",
    "#     mask_dir=mask_dir,\n",
    "#     sam_model=sam_model,\n",
    "#     file_list=train_files,\n",
    "#     device=CONFIG['device']\n",
    "# )\n",
    "\n",
    "# val_dataset = SegmentationDataset(\n",
    "#     image_dir=image_dir,\n",
    "#     mask_dir=mask_dir,\n",
    "#     sam_model=sam_model,\n",
    "#     file_list=val_files,\n",
    "#     device=CONFIG['device']\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=CONFIG['batch_size'],\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=CONFIG['batch_size'],\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# # 配置日志和保存目录\n",
    "# log_dir = os.path.join(CONFIG['log_dir_base'], CONFIG['dataset_name'])\n",
    "# save_dir = os.path.join(CONFIG['save_dir_base'], CONFIG['dataset_name'])\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     filename=os.path.join(log_dir, CONFIG['log_file']),\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "# )\n",
    "\n",
    "# # 学习率调度器\n",
    "# num_epochs = CONFIG['num_epochs']\n",
    "# warmup_epochs = 3\n",
    "# min_lr_factor = 0.01\n",
    "\n",
    "# def lr_lambda(epoch):\n",
    "#     if epoch < warmup_epochs:\n",
    "#         return float((epoch + 1) / warmup_epochs)\n",
    "#     else:\n",
    "#         cosine_decay = 0.5 * (1 + math.cos((epoch - warmup_epochs) * math.pi / (num_epochs - warmup_epochs)))\n",
    "#         return float(min_lr_factor + (1 - min_lr_factor) * cosine_decay)\n",
    "\n",
    "# scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# # 初始化最佳评分\n",
    "# best_composite_score = float('-inf')\n",
    "# best_epoch = 0\n",
    "\n",
    "# weights = CONFIG['metric_weights']\n",
    "# AUX_WEIGHT = CONFIG['aux_weight']\n",
    "\n",
    "# # 训练循环\n",
    "# for epoch in range(num_epochs):\n",
    "#     segmentation_head.train()\n",
    "#     auxiliary_classifiers.train()\n",
    "#     sam_model.train()\n",
    "\n",
    "#     total_loss = 0\n",
    "#     num_batches = 0\n",
    "\n",
    "#     for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\"):\n",
    "#         images = images.to(CONFIG['device'])\n",
    "#         masks = masks.to(CONFIG['device']).unsqueeze(1)\n",
    "\n",
    "#         if images.dim() != 4 or masks.dim() != 4:\n",
    "#             logging.error(f\"Invalid input dimensions: images {images.shape}, masks {masks.shape}\")\n",
    "#             continue\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             image_embedding, inter_features = sam_model.image_encoder.forward_inter(images)\n",
    "#         seg_head_logits = segmentation_head(image_embedding, inter_features)\n",
    "\n",
    "#         prompts = process_class_logits(seg_head_logits)\n",
    "#         final_predictions = predict_masks_batch(sam_model, image_embedding, prompts, CONFIG['device'])\n",
    "\n",
    "#         loss, seg_loss_val, sam_loss_val, aux_loss_val = compute_loss(\n",
    "#             seg_head_logits=seg_head_logits,\n",
    "#             final_predictions=final_predictions,\n",
    "#             masks=masks,\n",
    "#             loss_fn=loss_fn,\n",
    "#             aux_classifiers=auxiliary_classifiers,\n",
    "#             inter_features=inter_features,\n",
    "#             selected_aux_layers=selected_aux_layers,\n",
    "#             aux_weight=AUX_WEIGHT,\n",
    "#             seg_weight=1.0,\n",
    "#             sam_weight=1.0,\n",
    "#             loss_weights=[1,1]\n",
    "#         )\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         num_batches += 1\n",
    "\n",
    "#     avg_train_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "#     # 验证阶段\n",
    "#     segmentation_head.eval()\n",
    "#     auxiliary_classifiers.eval()\n",
    "#     sam_model.eval()\n",
    "\n",
    "#     val_loss = 0\n",
    "#     num_val_batches = 0\n",
    "#     global_metrics_val = initialize_metrics()       # 最终预测指标\n",
    "#     global_metrics_val_seg = initialize_metrics()   # 分割头输出指标\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Validation]\"):\n",
    "#             images = images.to(CONFIG['device'])\n",
    "#             masks = masks.to(CONFIG['device']).unsqueeze(1)\n",
    "\n",
    "#             if images.dim() != 4 or masks.dim() != 4:\n",
    "#                 logging.error(f\"Invalid input dimensions: images {images.shape}, masks {masks.shape}\")\n",
    "#                 continue\n",
    "\n",
    "#             image_embedding, inter_features = sam_model.image_encoder.forward_inter(images)\n",
    "#             seg_head_logits = segmentation_head(image_embedding, inter_features)\n",
    "\n",
    "#             prompts = process_class_logits(seg_head_logits)\n",
    "#             final_predictions = predict_masks_batch(sam_model, image_embedding, prompts, CONFIG['device'])\n",
    "\n",
    "#             loss, seg_loss_val, sam_loss_val, _ = compute_loss(\n",
    "#                 seg_head_logits=seg_head_logits,\n",
    "#                 final_predictions=final_predictions,\n",
    "#                 masks=masks,\n",
    "#                 loss_fn=loss_fn,\n",
    "#                 aux_classifiers=None,\n",
    "#                 inter_features=None,\n",
    "#                 selected_aux_layers=None,\n",
    "#                 aux_weight=0.0,\n",
    "#                 seg_weight=1.0,\n",
    "#                 sam_weight=1.0,\n",
    "#                 loss_weights=[1,1]\n",
    "#             )\n",
    "#             val_loss += loss.item()\n",
    "#             num_val_batches += 1\n",
    "\n",
    "#             # 计算分割头输出指标\n",
    "#             preds_seg = torch.sigmoid(seg_head_logits).cpu().numpy()  # 分割头输出的预测\n",
    "#             preds_final = torch.sigmoid(final_predictions).cpu().numpy()  # 最终预测结果\n",
    "#             masks_np = masks.cpu().numpy()\n",
    "\n",
    "#             for p_seg, p_final, m_gt in zip(preds_seg, preds_final, masks_np):\n",
    "#                 # 分割头输出指标\n",
    "#                 accumulate_metrics(p_seg[0], m_gt, global_metrics_val_seg)\n",
    "#                 # 最终预测指标\n",
    "#                 accumulate_metrics(p_final[0], m_gt, global_metrics_val)\n",
    "\n",
    "#     # 计算评价指标（分割头输出）\n",
    "#     tp_seg = global_metrics_val_seg['tp']\n",
    "#     fp_seg = global_metrics_val_seg['fp']\n",
    "#     fn_seg = global_metrics_val_seg['fn']\n",
    "#     intersection_seg = global_metrics_val_seg['intersection']\n",
    "#     union_seg = global_metrics_val_seg['union']\n",
    "\n",
    "#     iou_seg = intersection_seg / (union_seg + 1e-6)\n",
    "#     precision_seg = tp_seg / (tp_seg + fp_seg + 1e-6)\n",
    "#     recall_seg = tp_seg / (tp_seg + fn_seg + 1e-6)\n",
    "#     f1_seg = (2 * precision_seg * recall_seg) / (precision_seg + recall_seg + 1e-6)\n",
    "\n",
    "#     # 计算评价指标（最终预测）\n",
    "#     tp = global_metrics_val['tp']\n",
    "#     fp = global_metrics_val['fp']\n",
    "#     fn = global_metrics_val['fn']\n",
    "#     intersection = global_metrics_val['intersection']\n",
    "#     union = global_metrics_val['union']\n",
    "\n",
    "#     iou = intersection / (union + 1e-6)\n",
    "#     precision = tp / (tp + fp + 1e-6)\n",
    "#     recall = tp / (tp + fn + 1e-6)\n",
    "#     f1 = (2 * precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "#     avg_iou_seg = iou_seg\n",
    "#     avg_precision_seg = precision_seg\n",
    "#     avg_recall_seg = recall_seg\n",
    "#     avg_f1_seg = f1_seg\n",
    "\n",
    "#     avg_iou = iou\n",
    "#     avg_precision = precision\n",
    "#     avg_recall = recall\n",
    "#     avg_f1 = f1\n",
    "\n",
    "#     composite_score = (\n",
    "#         avg_iou * weights['iou'] +\n",
    "#         avg_f1 * weights['f1'] +\n",
    "#         avg_precision * weights['precision'] +\n",
    "#         avg_recall * weights['recall']\n",
    "#     )\n",
    "\n",
    "#     avg_val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "\n",
    "#     log_message = (\n",
    "#         f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "#         f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "#         f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "#         f\"(SegHead) IoU: {avg_iou_seg:.4f}, F1: {avg_f1_seg:.4f}, Precision: {avg_precision_seg:.4f}, Recall: {avg_recall_seg:.4f}, \"\n",
    "#         f\"(Final) IoU: {avg_iou:.4f}, F1: {avg_f1:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, \"\n",
    "#         f\"Composite Score: {composite_score:.4f}, \"\n",
    "#         f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "#     )\n",
    "#     logging.info(log_message)\n",
    "#     print(log_message)\n",
    "\n",
    "#     # 保存最佳模型\n",
    "#     if composite_score > best_composite_score:\n",
    "#         best_composite_score = composite_score\n",
    "#         best_epoch = epoch + 1\n",
    "\n",
    "#         strategy = 'MLA'\n",
    "#         checkpoint_path = os.path.join(save_dir, f\"{CONFIG['save_prefix']}_{strategy}.pth\")\n",
    "\n",
    "#         # 仅保存分割头模型和适配器参数\n",
    "#         save_dict = {\n",
    "#             'segmentation_head': segmentation_head.state_dict(),\n",
    "#             'auxiliary_classifiers': auxiliary_classifiers.state_dict(),\n",
    "#             'adapters': sam_model.image_encoder.adapters.state_dict()\n",
    "#         }\n",
    "#         torch.save(save_dict, checkpoint_path)\n",
    "\n",
    "#         save_message = (\n",
    "#             f\"Best model saved at epoch {best_epoch} with Composite Score {best_composite_score:.4f} using {strategy} strategy\"\n",
    "#         )\n",
    "#         logging.info(save_message)\n",
    "#         print(save_message)\n",
    "\n",
    "#     # 更新学习率\n",
    "#     scheduler.step()\n",
    "\n",
    "# logging.info(\"训练完成\")\n",
    "# print(\"训练完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cad367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import logging\n",
    "# from tqdm import tqdm\n",
    "# from types import MethodType\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from segment_anything import sam_model_registry\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Configuration\n",
    "# CONFIG = {\n",
    "#     'dataset_name': 'CTS-Pore',\n",
    "#     'data_base_dir': 'datasets',\n",
    "#     'model_type': 'vit_l',\n",
    "#     'checkpoint': 'weights/sam_vit_l_0b3195.pth',\n",
    "#     'trained_model_path': 'logs/CTS-Pore/best_model_MLA.pth',\n",
    "#     'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "#     'batch_size': 2,\n",
    "#     'save_predictions': True,  # Whether to save prediction masks\n",
    "#     'predictions_dir': 'predictions'  # Directory to save predictions\n",
    "# }\n",
    "\n",
    "# # Setup paths\n",
    "# image_dir = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'images')\n",
    "# mask_dir = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'masks')\n",
    "# test_txt = os.path.join(CONFIG['data_base_dir'], CONFIG['dataset_name'], 'test.txt')\n",
    "\n",
    "# # Create predictions directory if needed\n",
    "# if CONFIG['save_predictions']:\n",
    "#     predictions_dir = os.path.join(CONFIG['predictions_dir'], CONFIG['dataset_name'])\n",
    "#     os.makedirs(predictions_dir, exist_ok=True)\n",
    "\n",
    "# # Setup logging\n",
    "# log_dir = os.path.join('evaluation_logs', CONFIG['dataset_name'])\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# log_file = f'evaluation_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "# logging.basicConfig(\n",
    "#     filename=os.path.join(log_dir, log_file),\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "# )\n",
    "\n",
    "# # Model components (same as training)\n",
    "# class Adapter(nn.Module):\n",
    "#     def __init__(self, input_dim, reduction_factor=256):\n",
    "#         super(Adapter, self).__init__()\n",
    "#         self.down_project = nn.Linear(input_dim, input_dim // reduction_factor)\n",
    "#         self.activation = nn.GELU()\n",
    "#         self.up_project = nn.Linear(input_dim // reduction_factor, input_dim)\n",
    "#         self.layer_norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "#         x = self.layer_norm(x)\n",
    "#         x = self.down_project(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.up_project(x)\n",
    "#         return x + residual\n",
    "\n",
    "# class SegmentationHead(nn.Module):\n",
    "#     def __init__(self, in_channels, intermediate_channels, out_channels=1, align_corners=False):\n",
    "#         super(SegmentationHead, self).__init__()\n",
    "#         self.align_corners = align_corners\n",
    "\n",
    "#         self.mla_branches = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(1024, 512, kernel_size=3, padding=1, stride=1),\n",
    "#                 nn.BatchNorm2d(512),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Conv2d(512, 256, kernel_size=3, padding=1, stride=1),\n",
    "#                 nn.BatchNorm2d(256),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ) for _ in range(4)\n",
    "#         ])\n",
    "\n",
    "#         self.mla_image_branch = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 256, kernel_size=1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "        \n",
    "#         self.mla_classifier_branch = nn.Sequential(\n",
    "#             nn.Conv2d(256 * 5, intermediate_channels, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(intermediate_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(intermediate_channels, out_channels, kernel_size=1, stride=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, image_embedding, inter_features):\n",
    "#         selected_features = [inter_features[i] for i in [5, 11, 17, 23]]\n",
    "#         selected_features = [feat.permute(0, 3, 1, 2) for feat in selected_features]\n",
    "\n",
    "#         processed_features = []\n",
    "#         for i, feat in enumerate(selected_features):\n",
    "#             branch = self.mla_branches[i]\n",
    "#             x_feat = branch(feat)\n",
    "#             x_feat = F.interpolate(x_feat, scale_factor=4, mode='bilinear', align_corners=self.align_corners)\n",
    "#             processed_features.append(x_feat)\n",
    "\n",
    "#         img_feat = self.mla_image_branch(image_embedding)\n",
    "#         img_feat = F.interpolate(img_feat, scale_factor=4, mode='bilinear', align_corners=self.align_corners)\n",
    "#         processed_features.append(img_feat)\n",
    "\n",
    "#         aggregated = torch.cat(processed_features, dim=1)\n",
    "#         x = self.mla_classifier_branch(aggregated)\n",
    "#         x = F.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=self.align_corners)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# # Modified forward_inter for SAM\n",
    "# def forward_inter(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#     x = self.patch_embed(x)\n",
    "#     if self.pos_embed is not None:\n",
    "#         x = x + self.pos_embed\n",
    "\n",
    "#     inter_features = []\n",
    "#     for i, blk in enumerate(self.blocks):\n",
    "#         x = blk(x)\n",
    "#         x = self.adapters[i](x)\n",
    "#         inter_features.append(x)\n",
    "\n",
    "#     x = self.neck(x.permute(0, 3, 1, 2))\n",
    "#     return x, inter_features\n",
    "\n",
    "# # Data loading helpers\n",
    "# def read_split_files(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         return f.read().strip().split('\\n')\n",
    "\n",
    "# class SegmentationDataset(Dataset):\n",
    "#     def __init__(self, image_dir, mask_dir, sam_model, file_list, mask_size=(1024, 1024), device='cpu'):\n",
    "#         self.image_dir = image_dir\n",
    "#         self.mask_dir = mask_dir\n",
    "#         self.sam_model = sam_model\n",
    "#         self.mask_size = mask_size\n",
    "#         self.device = device\n",
    "#         self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png') and f.replace('.png', '') in file_list]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image_file = self.image_files[idx]\n",
    "#         image_path = os.path.join(self.image_dir, image_file)\n",
    "#         image = cv2.imread(image_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#         mask_path = os.path.join(self.mask_dir, image_file)\n",
    "#         mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "#         mask = cv2.resize(mask, self.mask_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#         input_image_torch = torch.as_tensor(image, dtype=torch.float32).to(self.device)\n",
    "#         input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()\n",
    "\n",
    "#         input_image = self.sam_model.preprocess(input_image_torch.to(self.device))\n",
    "#         mask = torch.as_tensor(mask, dtype=torch.float32).to(self.device)\n",
    "\n",
    "#         return input_image, mask, image_file\n",
    "\n",
    "# # Helper functions for processing predictions\n",
    "# def process_class_logits(class_logits):\n",
    "#     probs = torch.sigmoid(class_logits)\n",
    "#     binary_masks = (probs > 0.5).cpu().numpy().astype(np.uint8)\n",
    "#     batch_results = []\n",
    "    \n",
    "#     for batch_idx in range(binary_masks.shape[0]):\n",
    "#         current_mask = binary_masks[batch_idx, 0, :, :]\n",
    "#         num_labels, labels = cv2.connectedComponents(current_mask)\n",
    "#         sample_results = []\n",
    "        \n",
    "#         for label in range(1, num_labels):\n",
    "#             current_component = (labels == label).astype(np.uint8)\n",
    "#             y_coords, x_coords = np.nonzero(current_component)\n",
    "            \n",
    "#             if len(y_coords) == 0:\n",
    "#                 continue\n",
    "                \n",
    "#             min_x, max_x = np.min(x_coords), np.max(x_coords)\n",
    "#             min_y, max_y = np.min(y_coords), np.max(y_coords)\n",
    "            \n",
    "#             mask = np.zeros_like(current_component, dtype=np.uint8)\n",
    "#             mask[min_y:max_y+1, min_x:max_x+1] = current_component[min_y:max_y+1, min_x:max_x+1]\n",
    "            \n",
    "#             mask_resized = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "#             sample_results.append({\n",
    "#                 'bbox': [min_x, min_y, max_x, max_y],\n",
    "#                 'mask': mask_resized\n",
    "#             })\n",
    "            \n",
    "#         batch_results.append(sample_results)\n",
    "#     return batch_results\n",
    "\n",
    "# def predict_masks_batch(sam_model, image_embeddings, batch_results, device='cuda'):\n",
    "#     sam_model.eval()\n",
    "#     final_predictions = []\n",
    "    \n",
    "#     for idx, sample_results in enumerate(batch_results):\n",
    "#         if not sample_results:\n",
    "#             final_predictions.append(torch.zeros((1, 1, 1024, 1024), device=device))\n",
    "#             continue\n",
    "\n",
    "#         current_image_embedding = image_embeddings[idx:idx+1]\n",
    "#         sparse_embeddings_list = []\n",
    "#         dense_embeddings_list = []\n",
    "        \n",
    "#         for mask_info in sample_results:\n",
    "#             box = torch.tensor(mask_info['bbox'], dtype=torch.float, device=device).unsqueeze(0)\n",
    "#             mask = torch.from_numpy(mask_info['mask']).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "#             sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "#                 points=None,\n",
    "#                 boxes=box,\n",
    "#                 masks=mask\n",
    "#             )\n",
    "            \n",
    "#             sparse_embeddings_list.append(sparse_embeddings)\n",
    "#             dense_embeddings_list.append(dense_embeddings)\n",
    "        \n",
    "#         if not sparse_embeddings_list:\n",
    "#             final_predictions.append(torch.zeros((1, 1, 1024, 1024), device=device))\n",
    "#             continue\n",
    "            \n",
    "#         sparse_embeddings_all = torch.cat(sparse_embeddings_list, dim=0)\n",
    "#         dense_embeddings_all = torch.cat(dense_embeddings_list, dim=0)\n",
    "        \n",
    "#         low_res_masks, _ = sam_model.mask_decoder(\n",
    "#             image_embeddings=current_image_embedding,\n",
    "#             image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "#             sparse_prompt_embeddings=sparse_embeddings_all,\n",
    "#             dense_prompt_embeddings=dense_embeddings_all,\n",
    "#             multimask_output=False,\n",
    "#         )\n",
    "        \n",
    "#         resized_masks = F.interpolate(\n",
    "#             low_res_masks,\n",
    "#             size=(1024, 1024),\n",
    "#             mode='bilinear',\n",
    "#             align_corners=False\n",
    "#         )\n",
    "        \n",
    "#         merged_mask = torch.max(resized_masks, dim=0)[0]\n",
    "#         final_predictions.append(merged_mask.unsqueeze(0))\n",
    "        \n",
    "#     return torch.cat(final_predictions, dim=0)\n",
    "\n",
    "# def calculate_metrics(preds, targets, threshold=0.5):\n",
    "#     preds_binary = (preds > threshold).astype(np.uint8)\n",
    "#     targets_binary = (targets > threshold).astype(np.uint8)\n",
    "    \n",
    "#     tp = np.logical_and(preds_binary == 1, targets_binary == 1).sum()\n",
    "#     fp = np.logical_and(preds_binary == 1, targets_binary == 0).sum()\n",
    "#     fn = np.logical_and(preds_binary == 0, targets_binary == 1).sum()\n",
    "    \n",
    "#     intersection = tp\n",
    "#     union = np.logical_or(preds_binary == 1, targets_binary == 1).sum()\n",
    "    \n",
    "#     iou = intersection / (union + 1e-6)\n",
    "#     precision = tp / (tp + fp + 1e-6)\n",
    "#     recall = tp / (tp + fn + 1e-6)\n",
    "#     f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    \n",
    "#     return {\n",
    "#         'iou': iou,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall,\n",
    "#         'f1': f1,\n",
    "#         'tp': tp,\n",
    "#         'fp': fp,\n",
    "#         'fn': fn\n",
    "#     }\n",
    "\n",
    "# def main():\n",
    "#     # Initialize models\n",
    "#     sam_model = sam_model_registry[CONFIG['model_type']](checkpoint=CONFIG['checkpoint'])\n",
    "#     sam_model.to(CONFIG['device'])\n",
    "    \n",
    "#     # Initialize segmentation head\n",
    "#     segmentation_head = SegmentationHead(\n",
    "#         in_channels=256,\n",
    "#         intermediate_channels=256,\n",
    "#         out_channels=1,\n",
    "#         align_corners=False\n",
    "#     ).to(CONFIG['device'])\n",
    "    \n",
    "#     # Set up forward_inter method\n",
    "#     sam_model.image_encoder.forward_inter = MethodType(forward_inter, sam_model.image_encoder)\n",
    "    \n",
    "#     # Load trained weights\n",
    "#     checkpoint = torch.load(CONFIG['trained_model_path'], map_location=CONFIG['device'])\n",
    "#     segmentation_head.load_state_dict(checkpoint['segmentation_head'])\n",
    "    \n",
    "#     # Initialize and load adapters\n",
    "#     dummy_input = torch.zeros(1, 3, 1024, 1024).to(CONFIG['device'])\n",
    "#     with torch.no_grad():\n",
    "#         x = sam_model.image_encoder.patch_embed(dummy_input)\n",
    "#         input_dim = x.shape[-1]\n",
    "\n",
    "#     # Initialize adapters\n",
    "#     sam_model.image_encoder.adapters = nn.ModuleList([\n",
    "#         Adapter(input_dim=input_dim, reduction_factor=256)\n",
    "#         for _ in sam_model.image_encoder.blocks\n",
    "#     ]).to(CONFIG['device'])\n",
    "\n",
    "#     # Load adapter weights\n",
    "#     sam_model.image_encoder.adapters.load_state_dict(checkpoint['adapters'])\n",
    "\n",
    "#     # Load test data\n",
    "#     test_files = read_split_files(test_txt)\n",
    "#     test_dataset = SegmentationDataset(\n",
    "#         image_dir=image_dir,\n",
    "#         mask_dir=mask_dir,\n",
    "#         sam_model=sam_model,\n",
    "#         file_list=test_files,\n",
    "#         device=CONFIG['device']\n",
    "#     )\n",
    "\n",
    "#     test_loader = DataLoader(\n",
    "#         test_dataset,\n",
    "#         batch_size=CONFIG['batch_size'],\n",
    "#         shuffle=False\n",
    "#     )\n",
    "\n",
    "#     # Set models to evaluation mode\n",
    "#     segmentation_head.eval()\n",
    "#     sam_model.eval()\n",
    "\n",
    "#     # Initialize metric accumulators\n",
    "#     metrics_seghead = {\n",
    "#         'iou': [], 'precision': [], 'recall': [], 'f1': [],\n",
    "#         'tp': 0, 'fp': 0, 'fn': 0\n",
    "#     }\n",
    "#     metrics_final = {\n",
    "#         'iou': [], 'precision': [], 'recall': [], 'f1': [],\n",
    "#         'tp': 0, 'fp': 0, 'fn': 0\n",
    "#     }\n",
    "\n",
    "#     # Evaluation loop\n",
    "#     with torch.no_grad():\n",
    "#         for images, masks, filenames in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "#             masks = masks.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "#             # Forward pass through SAM encoder\n",
    "#             image_embedding, inter_features = sam_model.image_encoder.forward_inter(images)\n",
    "            \n",
    "#             # Get segmentation head predictions\n",
    "#             seg_head_logits = segmentation_head(image_embedding, inter_features)\n",
    "            \n",
    "#             # Process segmentation head outputs for SAM\n",
    "#             prompts = process_class_logits(seg_head_logits)\n",
    "            \n",
    "#             # Get final predictions from SAM\n",
    "#             final_predictions = predict_masks_batch(sam_model, image_embedding, prompts, CONFIG['device'])\n",
    "\n",
    "#             # Convert predictions to numpy and apply sigmoid\n",
    "#             seg_head_preds = torch.sigmoid(seg_head_logits).cpu().numpy()\n",
    "#             final_preds = torch.sigmoid(final_predictions).cpu().numpy()\n",
    "#             masks_np = masks.cpu().numpy()\n",
    "\n",
    "#             # Calculate metrics for each image in batch\n",
    "#             for idx in range(len(filenames)):\n",
    "#                 # Get metrics for segmentation head output\n",
    "#                 metrics_seg = calculate_metrics(seg_head_preds[idx, 0], masks_np[idx, 0])\n",
    "#                 metrics_seghead['iou'].append(metrics_seg['iou'])\n",
    "#                 metrics_seghead['precision'].append(metrics_seg['precision'])\n",
    "#                 metrics_seghead['recall'].append(metrics_seg['recall'])\n",
    "#                 metrics_seghead['f1'].append(metrics_seg['f1'])\n",
    "#                 metrics_seghead['tp'] += metrics_seg['tp']\n",
    "#                 metrics_seghead['fp'] += metrics_seg['fp']\n",
    "#                 metrics_seghead['fn'] += metrics_seg['fn']\n",
    "\n",
    "#                 # Get metrics for final predictions\n",
    "#                 metrics_fin = calculate_metrics(final_preds[idx, 0], masks_np[idx, 0])\n",
    "#                 metrics_final['iou'].append(metrics_fin['iou'])\n",
    "#                 metrics_final['precision'].append(metrics_fin['precision'])\n",
    "#                 metrics_final['recall'].append(metrics_fin['recall'])\n",
    "#                 metrics_final['f1'].append(metrics_fin['f1'])\n",
    "#                 metrics_final['tp'] += metrics_fin['tp']\n",
    "#                 metrics_final['fp'] += metrics_fin['fp']\n",
    "#                 metrics_final['fn'] += metrics_fin['fn']\n",
    "\n",
    "#                 # Save predictions if enabled\n",
    "#                 if CONFIG['save_predictions']:\n",
    "#                     # Save segmentation head predictions\n",
    "#                     seg_head_pred = (seg_head_preds[idx, 0] > 0.5).astype(np.uint8) * 255\n",
    "#                     cv2.imwrite(\n",
    "#                         os.path.join(predictions_dir, f\"{filenames[idx]}_seghead.png\"),\n",
    "#                         seg_head_pred\n",
    "#                     )\n",
    "\n",
    "#                     # Save final predictions\n",
    "#                     final_pred = (final_preds[idx, 0] > 0.5).astype(np.uint8) * 255\n",
    "#                     cv2.imwrite(\n",
    "#                         os.path.join(predictions_dir, f\"{filenames[idx]}_final.png\"),\n",
    "#                         final_pred\n",
    "#                     )\n",
    "\n",
    "#     # Calculate final metrics\n",
    "#     results = {\n",
    "#         'segmentation_head': {\n",
    "#             'mean_iou': np.mean(metrics_seghead['iou']),\n",
    "#             'mean_precision': np.mean(metrics_seghead['precision']),\n",
    "#             'mean_recall': np.mean(metrics_seghead['recall']),\n",
    "#             'mean_f1': np.mean(metrics_seghead['f1']),\n",
    "#             'global_precision': metrics_seghead['tp'] / (metrics_seghead['tp'] + metrics_seghead['fp'] + 1e-6),\n",
    "#             'global_recall': metrics_seghead['tp'] / (metrics_seghead['tp'] + metrics_seghead['fn'] + 1e-6),\n",
    "#             'global_f1': 2 * metrics_seghead['tp'] / (2 * metrics_seghead['tp'] + metrics_seghead['fp'] + metrics_seghead['fn'] + 1e-6)\n",
    "#         },\n",
    "#         'final_prediction': {\n",
    "#             'mean_iou': np.mean(metrics_final['iou']),\n",
    "#             'mean_precision': np.mean(metrics_final['precision']),\n",
    "#             'mean_recall': np.mean(metrics_final['recall']),\n",
    "#             'mean_f1': np.mean(metrics_final['f1']),\n",
    "#             'global_precision': metrics_final['tp'] / (metrics_final['tp'] + metrics_final['fp'] + 1e-6),\n",
    "#             'global_recall': metrics_final['tp'] / (metrics_final['tp'] + metrics_final['fn'] + 1e-6),\n",
    "#             'global_f1': 2 * metrics_final['tp'] / (2 * metrics_final['tp'] + metrics_final['fp'] + metrics_final['fn'] + 1e-6)\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     # Log results\n",
    "#     log_message = \"\\nEvaluation Results:\\n\" + \"-\" * 50 + \"\\n\"\n",
    "    \n",
    "#     log_message += \"\\nSegmentation Head Results:\\n\"\n",
    "#     for metric, value in results['segmentation_head'].items():\n",
    "#         log_message += f\"{metric}: {value:.4f}\\n\"\n",
    "    \n",
    "#     log_message += \"\\nFinal Prediction Results:\\n\"\n",
    "#     for metric, value in results['final_prediction'].items():\n",
    "#         log_message += f\"{metric}: {value:.4f}\\n\"\n",
    "\n",
    "#     logging.info(log_message)\n",
    "#     print(log_message)\n",
    "\n",
    "#     # Save results to JSON\n",
    "#     results_file = os.path.join(log_dir, 'evaluation_results.json')\n",
    "#     with open(results_file, 'w') as f:\n",
    "#         json.dump(results, f, indent=4)\n",
    "    \n",
    "#     print(f\"\\nResults saved to {results_file}\")\n",
    "#     if CONFIG['save_predictions']:\n",
    "#         print(f\"Predictions saved to {predictions_dir}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6a146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
